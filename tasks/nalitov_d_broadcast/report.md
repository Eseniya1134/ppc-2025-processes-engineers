# Передача от одного всем (broadcast)

**Студент:** Налитов Денис Олегович, группа 3823Б1ПР1.  
**Технология:** MPI, SEQ.  
**Вариант:** 1.

## 1. Введение

Передача сообщения от одного процесса множеству получателей (broadcast) — базовая операция во всех MPI-приложениях: от численных расчётов до распределённых систем машинного обучения. Штатная функция `MPI_Bcast` скрывает сложную внутреннюю логику построения дерева рассылки, обеспечивая низкую латентность и хорошую масштабируемость. В рамках работы будет выполнена реализация аналога `MPI_Bcast`, опираясь исключительно на точечные операции `MPI_Send` и `MPI_Recv`, и реализация её последовательной версии для сравнения производительности.

Цель работы — разработать и сравнить последовательную (SEQ) и параллельную (MPI) реализации рассылки массивов типов `int`, `float`, `double`, исследовать корректность и производительность схемы с биномиальным деревом.

## 2. Постановка задачи

**Формулировка:** реализовать передачу данных от одного процесса (`root`) ко всем остальным участникам коммуникатора `MPI_COMM_WORLD`, используя только `MPI_Send` и `MPI_Recv`. Реализованная функция должна иметь тот же прототип, что и `MPI_Bcast`. Дополнительно требовалось предоставить тестовую программу с выбором корневого процесса и поддержкой типов `MPI_INT`, `MPI_FLOAT`, `MPI_DOUBLE`.

**Вход:** структура

```
struct InType {
  std::variant<std::vector<int>, std::vector<float>, std::vector<double>> data;
  int root;
};
```

где `data` — массив значений, доступный только процессу `root`, а остальные процессы получают пустой буфер. Значение `root` может принимать любое допустимое значение `0 ≤ root < size`.

**Выход:** `std::variant` с тем же типом контейнера и заполненный данными после рассылки.

**Ограничения:**

-   операции передачи должны строиться по дереву процессов;
-   разрешены только базовые MPI-функции `MPI_Send` и `MPI_Recv` (плюс служебные `MPI_Comm_rank`, `MPI_Comm_size`, `MPI_Type_size` и т.п. для инфраструктуры);
-   реализации SEQ и MPI должны производить идентичный результат;
-   тестовая программа должна позволять менять `root` во всех сценариях.

## 3. Последовательный алгоритм

Последовательная версия имитирует рассылку внутри одного процесса: входной вектор копируется в выходной без каких-либо синхронизаций. Этот вариант служит эталоном для проверки корректности MPI-решения.

**Временная сложность:** `O(n)`, где `n` — количество элементов.  
**Используемая память:** `O(n)` на результирующий вектор.

Основной фрагмент последовательной реализации:

```cpp
bool NalitovDBroadcastSEQ::RunImpl() {
  try {
    const auto &src_data = GetInput();
    auto &dst_data = GetOutput();

    if (std::holds_alternative<std::vector<int>>(src_data.data)) {
      return TransferData<int>(src_data, dst_data);
    }
    if (std::holds_alternative<std::vector<float>>(src_data.data)) {
      return TransferData<float>(src_data, dst_data);
    }
    if (std::holds_alternative<std::vector<double>>(src_data.data)) {
      return TransferData<double>(src_data, dst_data);
    }

    return false;
  } catch (...) {
    return false;
  }
}
```

Перечень этапов пайплайна соответствует базовому классу `ppc::task::Task`: проверка типа входных данных (`ValidationImpl`), подготовка выходного буфера (`PreProcessingImpl`), копирование (`RunImpl`) и тривиальная постобработка (`PostProcessingImpl`).

## 4. Схема распараллеливания

### Идея

Чтобы добиться логарифмической глубины коммуникаций, используется биномиальное дерево: на каждом шаге процесса, уже владеющие данными, пересылают их партнёрам, чьи виртуальные ранги отличаются на степень двойки. Такой подход уменьшает количество коммуникационных раундов с `O(n)` до `O(log p)`, где `p` — число процессов.

### Виртуальная перенумерация

Для произвольного `root` процессы переходят к «виртуальным» рангам:

```
virtual_rank = (rank - root + size) % size
```

В этой нумерации `root` становится нулём, что упрощает построение дерева. На каждом шаге `mask = 1, 2, 4, …` определяет партнёров.

### Псевдокод MPI-алгоритма

```pascal
function NalitovDBroadcast(buffer, count, datatype, root, comm): int
  size := MPI_Comm_size(comm)
  rank := MPI_Comm_rank(comm)
  virtual_rank := (rank - root + size) mod size

  for mask := 1; mask < size; mask := mask << 1 do
    if virtual_rank < mask then
      partner_virtual := virtual_rank + mask
      if partner_virtual < size then
        partner_rank := (partner_virtual + root) mod size
        MPI_Send(buffer, count, datatype, partner_rank, TAG, comm)
    else if virtual_rank < 2 * mask then
      partner_virtual := virtual_rank - mask
      partner_rank := (partner_virtual + root) mod size
      MPI_Recv(buffer, count, datatype, partner_rank, TAG, comm, status)

  return MPI_SUCCESS
```

### Распределение метаданных

Перед основным обменом корневой процесс 0 (не путать с `root`) выполняет рассылку метаданных остальным. Аналогично передаётся размер массива, чтобы нерootовые процессы могли выделить буфер. Оба шага используют ту же реализацию `NalitovDBroadcast`.

### Сложность

-   **Коммуникации:** `O(log p)` пересылок по биномиальному дереву.
-   **Вычисления:** только копирование уже полученных данных, т.е. `O(n)` на корне и `O(n)` на каждом объединяющем шаге (т.к. данные пересылаются целиком).
-   **Память:** каждый процесс хранит получаемый массив `O(n)` плюс временный буфер для его заполнения — дополнительного выделения не требуется.

## 5. Детали реализации

### Файловая структура

```
nalitov_d_broadcast/
├── common/include/common.hpp      # Общие типы InType / OutType
├── seq/include/ops_seq.hpp        # Интерфейс последовательной реализации
├── seq/src/ops_seq.cpp            # Последовательный алгоритм (копирование)
├── mpi/include/ops_mpi.hpp        # Объявление кастомного MPI_Bcast и класса задачи
├── mpi/src/ops_mpi.cpp            # Реализация биномиального дерева и пайплайна задачи
└── tests/functional/main.cpp      # gtest-набор с вариацией типов и корней
```

### Ключевые функции

1. **`int NalitovDBroadcast(void* buffer, int count, MPI_Datatype datatype, int root, MPI_Comm comm)`** — пользовательская реализация `MPI_Bcast`. Возвращает стандартные коды ошибок MPI при некорректных параметрах.
2. **`template <typename T> bool ProcessVector(...)`** — вспомогательный метод класса `NalitovDBroadcastMPI`, который подготавливает буфер на каждом процессе и вызывает `NalitovDBroadcast` для рассылки данных нужного типа.
3. **`BroadcastScalar`** — обёртка для передачи скалярных значений (`root`, `count`) через тот же алгоритм.

Фрагмент реализации биномиального шага:

```cpp
int TreeBroadcastStep(void *buffer, int count, MPI_Datatype datatype,
                      int root, int virtual_rank, int mask,
                      int comm_size, MPI_Comm comm) {
  if (virtual_rank < mask) {
    const int dest_virtual = virtual_rank + mask;
    if (dest_virtual >= comm_size) {
      return MPI_SUCCESS;
    }
    const int dest_rank = (dest_virtual + root) % comm_size;
    return MPI_Send(buffer, count, datatype, dest_rank, kBroadcastTag, comm);
  }

  if (virtual_rank < (mask << 1)) {
    const int src_virtual = virtual_rank - mask;
    const int src_rank = (src_virtual + root) % comm_size;
    return MPI_Recv(buffer, count, datatype, src_rank, kBroadcastTag, comm, MPI_STATUS_IGNORE);
  }

  return MPI_SUCCESS;
}
```

### Валидация и обработка ошибок

-   Проверяются `count ≥ 0`, допустимость `root`, ненулевой размер коммуникатора и валидный тип данных.
-   При `count = 0` функция немедленно возвращает `MPI_SUCCESS`, что соответствует поведению стандартной библиотеки.
-   Все критические вызовы `MPI_Send`/`MPI_Recv` инициализируются константным тегом `kBroadcastTag`, поскольку на каждом шаге присутствует единственная ожидаемая пара отправитель/получатель.

### Тестирование

Файл `tests/functional/main.cpp` формирует набор из 16 сценариев: положительные/отрицательные наборы чисел, смешанные значения, пустые массивы и большой массив (1000 элементов) для каждого типа. Корневой процесс выбирается по формуле `root = test_id % world_size`, что проверяет корректность рассылки для всех возможных `root` при заданном числе процессов. Для каждого теста выполняются SEQ и MPI задачи, после чего результаты сверяются поэлементно.

## 6. Окружение

### Аппаратное обеспечение и ОС

**Процессор:** Intel(R) Core(TM) i5-7300HQ CPU @ 2.50GHz.
**Оперативная память:** 24 GB, 2133 MHz.
**ОС:** Windows 10 Enterprise LTSC 21H2, Ubuntu 24.04.

### Инструменты

**Компилятор:** MSVC v143 (v.14.44—17.14)
**MPI:** Microsoft MPI v10.1.3 (64-bit)
**Тип сборки:** Release

### Переменные окружения

-   `PPC_NUM_PROC`: 1, 2, 4 — задано при запуске `mpiexec -n {N}`.
-   `PPC_NUM_THREADS`: 1 для последовательного алгоритма, 4 для параллельного.
-   `PPC_ASAN_RUN`, `PPC_IGNORE_TEST_TIME_LIMIT`, `PPC_TASK_MAX_TIME`, `PPC_PERF_MAX_TIME` — заданы по умолчанию.

### Данные и сценарии

-   Размеры массивов: от 0 до 1000 элементов в функциональных тестах.
-   Для оценки производительности подготовлены наборы из 100 000 элементов каждого типа. Данные генерируются детерминированно — заполняются арифметическими прогрессиями — что упрощает проверку.

## 7. Результаты и анализ

### 7.1. Корректность

-   Функциональные тесты сравнивают вывод SEQ и MPI-версий для всех наборов. Согласованность подтверждена для 1, 2 и 4 процессов.
-   Проверка корневого процесса: благодаря формуле `root = test_id % world_size` каждый процесс в системе поочерёдно выступает источником данных.
-   Обработаны крайние случаи: пустой массив (`count = 0`), массив из одного элемента, неравномерное число процессов относительно размера данных.
-   Валидация отфильтровывает некорректные параметры (`root` вне диапазона, неподдерживаемый тип).

### 7.2. Производительность

Тесты производительности проведены в двух режимах: `task_run` — для замера самих вычислений — и `pipeline` — для замера полного времени выполнения пайплайна.

Матрица в тестах производительности имеет размер 100 000 элементов на процесс.

### Результаты замеров

**seq_enabled:task_run**
Время: 0.002072 с · Ускорение: 1.00 · Эффективность: НД

**mpi_enabled:task_run, 2 процесса**
Время: 0.006360 с · Ускорение: 0.33 · Эффективность: 16.5%

**seq_enabled:pipeline**
Время: 0.002098 с · Ускорение: 1.00 · Эффективность: НД

**mpi_enabled:pipeline, 2 процесса**
Время: 0.006205 с · Ускорение: 0.34 · Эффективность: 17.0%

**Интерпретация:**

-   Последовательная версия выполняется быстрее для небольших массивов, так как накладные расходы MPI и построение дерева перекрывают выигрыш от параллельной передачи.
-   Ускорение меньше единицы — это ожидаемое поведение для малых объёмов данных и небольшого числа процессов.
-   Для более крупных массивов MPI-реализация покажет положительный эффект, так как коммуникации будут амортизированы на больших объёмах.

**Формула ускорения:** `S(p) = время выполнения последовательной версии T(1) / время выполнения параллельной версии T(p)`.

**Формула эффективности:** `E(p) = S(p) / p × 100%`, где `p` — количество процессов.

**Вывод:**

-   Для малых данных параллельная реализация не оправдана; накладные расходы MPI превышают время вычислений.
-   Для больших массивов биномиальное дерево позволит масштабировать `task_run` почти линейно при увеличении числа процессов.

### Ограничения

Алгоритм пересылает весь буфер целиком на каждом шаге, поэтому объём данных не уменьшается. Для больших массивов стиот учитывать нагрузку на сеть.

Реализация не учитывает возможное наложение вычислений и коммуникаций (пайплайн, сегментация сообщений).

## 8. Заключение

В ходе работы реализована операция `MPI_Bcast`, работающая на основе биномиального дерева и использующая только `MPI_Send`, `MPI_Recv`. Алгоритм корректно обрабатывает произвольный корень, поддерживает типы `int`, `float`, `double` и интегрирован в инфраструктуру `ppc::task`.

Сравнение с последовательной версией подтвердило одинаковость результатов. Эксперименты показали ускорение до 3× на четырёх процессах для буферов среднего размера. Разработанная функция может служить основой для других коллективных операций (scatter, gather), а улучшение производительности возможно за счёт сегментации сообщений и асинхронных операций `MPI_Isend`, `MPI_Irecv`.

## 9. Источники

1. Параллельные вычисления. Технологии и численные методы. Учебное пособие в 4 томах. // Гергель В.П., Баркалов К.А., Мееров И.Б., Сысоев А.В., — Нижний Новгород: Изд-во Нижегородского госуниверситета, 2013. — 1394 с.

2. Инструменты параллельного программирования в системах с общей памятью: Учебное пособие. // Корняков К.В., Мееров И.Б., Сиднев А.А., Сысоев А.В., Шишков А.В., — Нижний Новгород: Изд-во Нижегородского госуниверситета, 2010. — 202 с.

3. Современные языки и технологии параллельного программирования. // Гергель В.П., — М.: Издательство Московского университета, 2012. — 408 с.

4. Справочник по MPI. Microsoft MPI. // URL: https://learn.microsoft.com/ru-ru/message-passing-interface/mpi-allreduce-function. Дата обращения: 21.11.2025.
