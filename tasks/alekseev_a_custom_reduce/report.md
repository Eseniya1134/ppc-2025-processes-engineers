# Передача от всех одному (reduce)

- **Студент:** Алексеев Артемий Алексеевич, группа 3823Б1ПР2
- **Технология:** SEQ | MPI
- **Вариант:** №2

## 1. Введение

**Цель работы:** Реализовать кастомный reduce, который бы выполнял идентичный функционал, как и MPI_REDUCE из библиотеки mpi.

**Задачи:**
1. Реализовать последовательную версию функции reduce.
2. Реализовать параллельную версию с использованием технологии MPI функции reduce.
3. Провести сравнительный анализ производительности и эффективности обеих реализаций.

## 2. Постановка задачи

**Задача**: Реализовать функцию reduce.

**Описание метода решения:** Реализация с использованием ReduceTree

**Входные данные:**
- struct InType — входной вектор + число, характеризующая root

**Выходные данные:**
- result - сумма элементов вектора 

**Ограничения:** 
- Root должен быть больше нуля


## 3. Описание алгоритма (последовательного)
**Алгоритм последовательного вычисления:**
1. **Суммирование в цикле**

**Реализация на C++:**

```cpp
const auto &input = GetInput().data;
double result = 0.0;
int sz = static_cast<int>(input.size());
for(auto i = 0; i < sz; i++){
  result += input[i];
}
GetOutput() = result;
return true;
```
## 4. Схема распараллеливания (MPI)
**Алгоритм параллельного вычисления:**
Аналогично, с некоторыми дополнениями 
1. **Аналогичный способ стандартной реализации с использованием ReduceTree**
```cpp
bool AlekseevACustomReduceMPI::RunImpl() {
    double global_sum = 0.0;
    double* recv_buffer = world_rank_ == root_ ? &global_sum : nullptr;

    const int reduce_result = CustomReduceImpl(
        &local_sum_, recv_buffer, 1, MPI_DOUBLE, MPI_SUM, root_, MPI_COMM_WORLD);

    if (reduce_result != MPI_SUCCESS) {
        return false;
    }

    constexpr int kResultTag = 1;
    
    if (world_rank_ == root_) {
        GetOutput() = global_sum;
        
        for (int dest = 0; dest < world_size_; ++dest) {
            if (dest != root_) {
                MPI_Send(&global_sum, 1, MPI_DOUBLE, dest, kResultTag, MPI_COMM_WORLD);
            }
        }
    } else {
        MPI_Status status;
        MPI_Recv(&global_sum, 1, MPI_DOUBLE, root_, kResultTag, MPI_COMM_WORLD, &status);
        GetOutput() = global_sum;
    }

    return true;
}
```

**Схема распределения вычислений:**
- Аналогично стандартной функции reduce из mpi

## 5. Детали реализации
### 5.1. Структура реализации
```text
alekseev_a_custom_reduce
    ├───common
    │   └───include
    │           common.hpp - определение типов входных/выходных/тестовых данных
    │
    ├───mpi
    │   ├───include
    │   │       ops_mpi.hpp - заголовочный файл MPI-реализации
    │   │
    │   └───src
    │           ops_mpi.cpp - код MPI-реализации
    │
    ├───seq
    │   ├───include
    │   │       ops_seq.hpp - заголовочный файл SEQ-реализации
    │   │
    │   └───src
    │           ops_seq.cpp - код SEQ-реализации
    │
    └───tests
        ├───functional
        │       main.cpp - функциональные тесты
        │
        └───performance
                main.cpp - тесты производительности
```
### 5.2. Основные классы / функции
- `AlekseevACustomReduceSEQ` - последовательная реализация
- `AlekseevACustomReduceMPI` - параллельная реализация
### 5.3. Пространственная и временная сложности алгоритмов
- Последовательная версия:
    - Временная сложность - `O(N)`, где `N` - размер входного вектора;
    - Пространственная сложность - `O(N)`, где `N` - размер входного вектора.
- Параллельная версия:
    - Временная сложность - эквивалентно сложности стандартной MPI_REDUCE;
    - Пространственная сложность - эквивалентно сложности стандартной MPI_REDUCE.
## 6. Экспериментальная среда
### 6.1. Аппаратное обеспечение:
| Параметр | Значение                                                                               |
| -------- | -------------------------------------------------------------------------------------- |
| CPU      | Ryzen 5 5600                                                                           |
| RAM      | 16 GB DDR4                                                                             |     
### 6.2. Программное обеспечение:
| Параметр   | Значение                                               |
| ---------- | ------------------------------------------------------ |
| ОС         | Windows 11 Home + WSL                                  |
| MPI        | OpenMPI 3.1                                            |
| Компилятор | g++ 14.2.0                                             |
| Сборка     | Release                                                |
### 6.3. Тестовые данные
**Функциональные тесты:**\
Используют заранее подготовленный массив пар `вектор - описание тестового случая`. Корректность ответа проверяется в функции `CheckTestOutputData` с помощью вычисления ответа последовательным алгоритмом.
**Тесты производительности:**
Вектор размером 10'000'000 генерируется в цикле по индексам вектора с помощью `fmod(static_cast<double>(i) * kMultiplier, 10000.0)`
## 7. Результаты и обсуждение

### 7.1 Корректность
Проверка корректности реализована средствами Google Test:
- 20 функциональных тестов покрывают различные ситуации.
- Perf-тест проверяет, что результат валиден.

Реализация успешно проходит как функциональные тесты, так и перф-тесты.

### 7.2 Производительность
**Метрики:**
1. Абсолютное время выполнения вычислительной части алгоритма в миллисекундах;
2. Ускорение относительно последовательной версии;
3. Эффективность распараллеливания = `(ускорение / число процессов) * 100%`.

**Полученные результаты:**

| **Режим** | **Количество процессов** | **Время, с** | **Speedup** | **Efficiency** |
|-----------|--------------------------|--------------|-------------|----------------|
| SEQ       | 1                        | 0.0182       | 1.00        | N/A            |
| MPI       | 2                        | 0.0483       | 0.37        | 18,5%          |
| MPI       | 4                        | 0.0437       | 0.42        | 21,5%          |
| MPI       | 8                        | 0.1085       | 0.17        | 8,5%           |

## 8. Заключение
В рамках данной работы были успешно реализована функция reduce, выполняющая тот же функционал, что и стандартная функция MPI_REDUCE из библиотеки mpi. Проведенные эксперименты подтвердили значительное ускорение MPI-реализации и корректность работы обоих вариантов алгоритма.

## 9. Источники
1. [Презентация по курсу](https://learning-process.github.io/parallel_programming_slides/)