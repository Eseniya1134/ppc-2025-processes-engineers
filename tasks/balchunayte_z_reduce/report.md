# Передача от всех одному (reduce)

- Студент: Бальчунайте Злата Денисовна, группа 3823Б1ПР3  
- Технология: MPI + SEQ  
- Вариант: 2
## 1. Введение
Операция редукции (в частности суммирование) — одна из базовых операций параллельных программ: она используется в линейной алгебре, статистике, численных методах, обработке больших данных и т.д.  

Цель работы — реализовать свою версию `MPI_Reduce` на бинарном дереве (поддержка только `MPI_SUM` и типов `MPI_INT / MPI_FLOAT / MPI_DOUBLE`), интегрировать её в инфраструктуру PPC, а также сравнить последовательную и MPI-реализации по производительности.

## 2. Постановка задачи
Дан массив чисел:

\[
a = (a_1, a_2, \ldots, a_n)
\]

Необходимо вычислить глобальную сумму:

\[
S = \sum_{i=1}^{n} a_i
\]

### Ограничения:

Поддерживаются только:
- операция MPI_SUM;
- типы MPI_INT, MPI_FLOAT, MPI_DOUBLE.
Вход/выход:
- Input: вектор чисел и значение `root`.
- Output: одно число — сумма всех элементов.

Решение должно корректно работать в инфраструктуре PPC и проходить функциональные и перф-тесты.

## 3. Базовый алгоритм (последовательный)
Последовательный алгоритм — прямой проход по массиву:

1. Инициализация суммы: `sum = 0`.
2. Цикл от 0 до N–1.
3. Прибавление каждого элемента к сумме.
4. Возврат результата.

Алгоритм является эталоном для проверки MPI-версии и базовым временем для расчета ускорения.

## 4. Схема распараллеливания (MPI)

### Распределение данных

1. Процесс 0:
- получает входной вектор data и значение root из структуры Input;
- вычисляет global_size = data.size().
2. Значение global_size рассылается всем процессам через MPI_Bcast.
3. Строятся массивы:
- counts[rank] — количество элементов на каждом процессе,
- displs[rank] — смещения в исходном массиве.
4. Используется блочное распределение с учётом остатка.
5. Далее вызывается MPI_Scatterv, и каждый процесс получает свой подмассив в local_data_.

### Собственная реализация MPI_Reduce (бинарное дерево)
Для редукции используется схема бинарного дерева по виртуальным рангам:
\[
v_rank = (rank - root + size) % size
\]

На каждом шаге `step = 1, 2, 4, ...`:

- Если `v_rank % (2*step) == 0`, процесс принимает данные от соседа `v_rank + step`.
- Иначе процесс отправляет локальную сумму процессу `v_rank - step` и выходит.

В итоге корневой процесс получает конечную сумму.

### Вывод результатов
PPC требует, чтобы результат был доступен всем процессам. Поэтому root отправляет итоговую сумму всем остальным процессам через `MPI_Send`.

## 5. Детали реализации

### Структура файлов
tasks/balchunayte_z_reduce/
├── common/include/common.hpp
├── seq/include/ops_seq.hpp
├── seq/src/ops_seq.cpp
├── mpi/include/ops_mpi.hpp
├── mpi/src/ops_mpi.cpp
└── tests/

### Ключевые элементы
- `BalchunayteZReduceSEQ`: последовательная реализация.
- `BalchunayteZReduceMPI`:
  - разбиение данных,
  - вычисление локальной суммы,
  - вызов `BalchunayteZReduce`,
  - рассылка результата всем процессам.

Память используется только для локальных фрагментов, что позволяет масштабировать задачу.

## 6. Экспериментальная установка

### Аппаратное и программное окружение
- **CPU:** 11th Gen Intel(R) Core(TM) i5-1135G7 @ 2.40GHz  
- **Ядер / потоков:** 4 / 8  
- **ОЗУ:** 8 ГБ  
- **ОС:** Windows 10 22H2

### Инструменты
- **Компилятор:** MSVC + Clang-Format/Clang-Tidy  
- **MPI:** Microsoft MPI (MS-MPI)  
- **Сборка:** Release  

### Команды запуска
- Последовательная версия:  
  `.\ppc_perf_tests.exe --gtest_filter=RunModeTests/BalchunayteZReduceRunPerfTestsProcesses.*`
- MPI-версия:  
  `mpiexec -n <k> .\ppc_perf_tests.exe --gtest_filter=RunModeTests/BalchunayteZReduceRunPerfTestsProcesses.*`

### Данные
Входные данные автоматически генерируются тестовым фреймворком PPC.

## 7. Результаты и обсуждение

### 7.1 Проверка корректности
Корректность подтверждена:
- все SEQ тесты проходят без MPI-запуска;
- все MPI тесты успешно проходят при `mpiexec -n 2` и `mpiexec -n 4`;
- результаты SEQ и MPI совпадают;
- бинарное дерево корректно обрабатывает разные размеры массива.

Ошибок корректности нет.

### 7.2 Производительность

#### Последовательная версия
| Режим | Время (с) |
|-------|-----------|
| seq pipeline | 0.01103  |
| seq task_run | 0.01113  |

#### MPI: 2 процесса
| Режим | Время (с) |
|-------|-----------|
| mpi pipeline | 0.01614  |
| mpi task_run | 0.0000168  |

#### MPI: 4 процесса
| Режим | Время (с) |
|-------|-----------|
| mpi pipeline | 0.01521  |
| mpi task_run | 0.0000389  |

#### Ускорение

Базовое время: **0.01103 (seq pipeline)**

| Процессы | Время (с) | Ускорение | Эффективность |
|----------|-----------|-----------|---------------|
| 1 | 0.01103 | 1.00 | 100% |
| 2 | 0.01614 | 0.68 | 34% |
| 4 | 0.01521 | 0.73 | 18% |

### Обсуждение
- MPI `task_run` показывает крайне малое время — сама редукция очень быстрая.
- Pipeline-режим медленнее из-за:
  - Scatterv,
  - финальной рассылки результата,
  - небольшого размера входных данных,
  - работы на одном узле (MS-MPI → shared memory).
- Для крупных массивов и распределенных вычислений ускорение будет выше.

## 8. Заключение
В ходе работы:

- разработан собственный аналог `MPI_Reduce` на бинарном дереве;
- поддержаны типы `int`, `float`, `double` и операция `MPI_SUM`;
- реализованы SEQ и MPI варианты задачи суммирования;
- проведено тестирование на 1–4 процессах, функциональные и перф-тесты полностью пройдены;
- код приведён к стилю и проверен `clang-tidy`.

MPI-реализация демонстрирует правильную работу и ожидаемую производительность, ограниченную малыми размерами тестовых данных.

## 9. Источники
1. Учебные материалы курса PPC (лекции и практики)
2. Microsoft MPI Documentation   

## Приложение (опционально)
```cpp
// Схема бинарного уменьшения
for (int step = 1; step < size; step *= 2) {
    if (v_rank % (2 * step) == 0) {
        receive and add;
    } else {
        send and exit;
    }
}
