# Вычисление многомерных интегралов с использованием многошаговой схемы (метод трапеций)

- Студент: Точилин Евгений Дмитриевич
- Группа: 3823Б1ПР3
- Технология: MPI|SEQ
- Вариант: 8

## 1. Введение

Вычисление многомерных интегралов является одной из задач в вычислительной математике, которая находит применение в различных областях науки и техники. Точное аналитическое вычисление интегралов высокой размерности часто затруднительно, поэтому используются численные методы приближенного интегрирования.

Метод трапеций является одним из базовых методов численного интегрирования, который может быть эффективно распараллелен для ускорения вычислений. Целью данной лабораторной работы является реализация последовательного и параллельного алгоритмов вычисления многомерных интегралов методом трапеций с использованием технологии MPI.

## 2. Постановка задачи

Требуется вычислить многомерный интеграл функции f(x1, x2, ..., xd) по области интегрирования [a1, b1] x [a2, b2] x ... x [ad, bd] с использованием составной формулы трапеций.

Входные данные:
- Вектор нижних границ интегрирования lower_bounds = [a1, a2, ..., ad]
- Вектор верхних границ интегрирования upper_bounds = [b1, b2, ..., bd]
- Количество шагов разбиения num_steps для каждой размерности
- Функция f(x1, x2, ..., xd), подлежащая интегрированию

Выходные данные:
- Приближенное значение интеграла (вещественное число)

Ограничения:
- Размерности векторов lower_bounds и upper_bounds должны совпадать
- num_steps должен быть положительным числом
- Функция должна быть определена на всей области интегрирования
- Для каждой размерности должно выполняться условие ai <= bi

## 3. Базовый алгоритм

Метод трапеций для одномерного интеграла основан на приближении подынтегральной функции кусочно-линейной функцией. Для интервала [a, b] с разбиением на n равных частей формула трапеций имеет вид:

I = (h/2) * [f(a) + 2*f(a+h) + 2*f(a+2h) + ... + 2*f(b-h) + f(b)]

где h = (b - a) / n - шаг разбиения.

Для многомерного случая метод трапеций обобщается путем применения одномерной формулы по каждой размерности. Интеграл вычисляется как сумма значений функции во всех узлах сетки, умноженных на соответствующие весовые коэффициенты. Весовой коэффициент для узла зависит от того, находится ли узел на границе области интегрирования по каждой из размерностей. Если узел находится на границе по k размерностям, его вес умножается на 0.5^k.

Алгоритм последовательного вычисления:
1. Вычисление шагов разбиения для каждой размерности
2. Генерация всех узлов сетки 
3. Для каждого узла:
   - Вычисление координат точки
   - Определение весового коэффициента 
   - Вычисление значения функции и добавление к сумме с учетом веса
4. Умножение суммы на объем элементарной ячейки 

## 4. Описание последовательного алгоритма

Последовательный алгоритм реализован в классе TochilinEIntegralTrapeziumSEQ. Алгоритм работает следующим образом:

1. Валидация входных данных (ValidationImpl):
   - Проверка на пустоту векторов границ
   - Проверка совпадения размерностей векторов
   - Проверка положительности num_steps
   - Проверка наличия функции
   - Проверка корректности границ (ai <= bi)

2. Препроцессинг (PreProcessingImpl):
   - Сохранение входных параметров в приватные переменные класса
   - Инициализация результата нулем

3. Вычисление интеграла (RunImpl):
   - Вычисление шагов разбиения для каждой размерности
   - Определение общего количества узлов сетки
   - Последовательный перебор всех узлов сетки
   - Для каждого узла вычисление координат, весового коэффициента и значения функции
   - Накопление взвешенной суммы
   - Умножение суммы на объем элементарной ячейки

4. Постпроцессинг (PostProcessingImpl):
   - Сохранение результата в выходную переменную

Вычислительная сложность алгоритма составляет O((n+1)^d), где n - количество шагов, d - размерность интеграла. Это связано с необходимостью вычисления функции во всех узлах многомерной сетки.

## 5. Схема распараллеливания

Параллельный алгоритм реализован с использованием технологии MPI. Распараллеливание основано на декомпозиции данных - разделении узлов сетки между процессами.

Схема распределения работы:
1. Процесс с рангом 0 получает входные данные
2. Главный процесс вычисляет общее количество узлов сетки
3. Узлы распределяются между процессами с использованием блочного распределения с учетом остатка
4. Каждый процесс вычисляет частичный интеграл для своей порции узлов
5. Результаты собираются на главном процессе с помощью операции MPI_Reduce
6. Финальный результат рассылается всем процессам через MPI_Bcast

Коммуникационная схема:
- PreProcessingImpl: MPI_Bcast для рассылки размерности, количества шагов и границ интегрирования; MPI_Send/MPI_Recv для распределения диапазонов индексов
- RunImpl: MPI_Reduce для сбора частичных результатов на процесс 0
- PostProcessingImpl: MPI_Bcast для рассылки финального результата всем процессам

Топология коммуникаций: используется коммуникатор MPI_COMM_WORLD, где процесс 0 является корнем.

Распределение работы происходит следующим образом:
- Общее количество узлов total_points делится на количество процессов size
- Базовое количество узлов на процесс: base_points_per_proc = total_points / size
- Остаток remainder = total_points % size распределяется между первыми remainder процессами
- Каждый процесс получает диапазон индексов [start_idx, end_idx) для вычисления

## 6. Детали реализации

Структура кода:
- common/include/common.hpp - определение общих типов данных (IntegralInput, InType, OutType)
- seq/include/ops_seq.hpp - заголовочный файл последовательной реализации
- seq/src/ops_seq.cpp - реализация последовательного алгоритма
- mpi/include/ops_mpi.hpp - заголовочный файл параллельной реализации
- mpi/src/ops_mpi.cpp - реализация параллельного алгоритма

Ключевые классы и методы:
- TochilinEIntegralTrapeziumSEQ: последовательная реализация с методом ComputeIntegral()
- TochilinEIntegralTrapeziumMPI: параллельная реализация с методом ComputePartialIntegral()

Важные особенности реализации:
- Использование индексации узлов сетки через систему счисления с основанием (num_steps + 1) для эффективного перебора всех комбинаций
- Вычисление весовых коэффициентов на лету при обходе узлов
- Минимизация коммуникаций в RunImpl для точного измерения производительности вычислений

Граничные случаи:
- Обработка случая, когда количество узлов не делится нацело на количество процессов
- Корректная обработка граничных узлов с весовым коэффициентом 0.5
- Валидация входных данных на всех этапах

Использование памяти:
- Основная память расходуется на хранение векторов границ и промежуточных массивов
- Память для коммуникаций минимальна, так как используется передача только скалярных значений и небольших массивов

## 7. Экспериментальная установка

Оборудование и ОС:
- Процессор: AMD Ryzen 5 7500F
- Количество ядер: 6 (физических ядер) (3.70 GHz)
- ОЗУ: 32GB
- ОС: Windows 11 64-bit

- Компилятор: MSVC (Microsoft Visual C++) с поддержкой C++17
- Build type: Release
- MPI: OpenMPI 3.1
- Система сборки: CMake 4.1.3

Параметры запуска:
- Количество процессов: 1, 2, 4, 6, 8
- Тестовые данные: трехмерный интеграл функции f(x,y,z) = sin(x)*cos(y) + x^2 + y^2 + z^2 на области [0,1] x [0,1] x [0,1]
- Количество шагов: 1000 (для обеспечения достаточной вычислительной нагрузки)

Переменные окружения:
- Используются стандартные настройки проекта для MPI

## 8. Результаты и обсуждение

### 8.1. Проверка корректности

Корректность реализации проверялась с помощью набора функциональных тестов, покрывающих различные сценарии:
- Интегралы постоянных функций (проверка объема области)
- Линейные и квадратичные функции (сравнение с аналитическими решениями)
- Тригонометрические функции (sin, cos)
- Экспоненциальные функции
- Интегралы различной размерности (1D, 2D, 3D)
- Граничные случаи (один шаг, различные области интегрирования)
- Валидация входных данных (пустые границы, несовпадение размерностей, некорректные шаги, нулевая функция, инвертированные границы)

Всего было реализовано 36 тестов. Все тесты проходят, что подтверждает корректность реализации двух алгоритмов.

### 8.2. Производительность

Для оценки производительности использовались два режима измерения:
- task_run: измерение времени только выполнения RunImpl (основные вычисления)
- task_pipeline: измерение времени выполнения всего пайплайна (Validation, PreProcessing, RunImpl, PostProcessing)

Формулы для вычисления метрик:
- Ускорение (Speedup): S(p) = T(1) / T(p), где T(1) - время выполнения на 1 процессе, T(p) - время выполнения на p процессах
- Эффективность (Efficiency): E(p) = S(p) / p = T(1) / (p * T(p))

Результаты для режима task_run:

| Процессов | Время, с | Ускорение | Эффективность |
|-----------|----------|-----------|---------------|
| 1         | 0.1127   | 1.00      | N/A           |
| 2         | 0.0805   | 1.40      | 70.0%         |
| 4         | 0.0628   | 1.79      | 44.8%         |
| 6         | 0.0684   | 1.65      | 27.5%         |
| 8         | 0.0696   | 1.62      | 20.2%         |

Результаты для режима task_pipeline:

| Процессов | Время, с | Ускорение | Эффективность |
|-----------|----------|-----------|---------------|
| 1         | 0.1231   | 1.00      | N/A           |
| 2         | 0.0769   | 1.60      | 80.0%         |
| 4         | 0.0698   | 1.76      | 44.1%         |
| 6         | 0.0639   | 1.93      | 32.1%         |
| 8         | 0.0669   | 1.84      | 23.0%         |

Анализ результатов:

Режим task_run демонстрирует лучшее ускорение на 4 процессах (1.79), после чего производительность начинает снижаться. Это связано с тем, что при большом количестве процессов коммуникационные накладные расходы операции MPI_Reduce начинают преобладать над вычислительной работой. На 6 и 8 процессах время выполнения даже увеличивается по сравнению с 4 процессами, что указывает на достижение точки насыщения параллелизма.

Режим task_pipeline показывает максимальное ускорение на 6 процессах (1.93), что выше, чем в task_run. Это объясняется тем, что в pipeline режиме коммуникации в PreProcessing и PostProcessing выполняются параллельно с вычислениями, что позволяет лучше скрыть задержки коммуникаций. Однако эффективность все равно снижается с ростом количества процессов из-за увеличения времени на синхронизацию.

Эффективность параллелизации снижается с увеличением количества процессов: от 70-80% на 2 процессах до 20-23% на 8 процессах. Это предсказуемое поведение для задач с высокой коммуникационной нагрузкой. Основными узкими местами являются операция MPI_Reduce, требующая синхронизации всех процессов, и неравномерность распределения работы при некратном делении количества узлов на количество процессов.

## 9. Выводы

В ходе выполнения лабораторной работы были реализованы последовательный и параллельный алгоритмы вычисления многомерных интегралов методом трапеций. 

Что было реализовано:
- Успешно реализован алгоритм метода трапеций для многомерных интегралов
- Обеспечено покрытие тестами 
- Реализована эффективная схема распараллеливания с минимальными коммуникационными затратами

Ограничения:
- Эффективность параллелизации снижается при большом количестве процессов из-за коммуникационных накладных расходов

Возможные улучшения:
- Использование адаптивных методов интегрирования для уменьшения количества вычислений
- Оптимизация коммуникаций через группировку данных

## 10. Список литературы

1. MPI Forum. MPI: A Message-Passing Interface Standard. Version 4.0. https://www.mpi-forum.org/docs/mpi-4.0/mpi40-report.pdf

2. Самарский А.А., Гулин А.В.: Численные методы. М.: Наука, 1989 год

3. Open MPI Documentation: https://www.open-mpi.org/doc/

4. Сысоев А.В.: Лекции по параллельному программированию. ННГУ, 2025 год

## 11. Приложение

### 11.1. Последовательная реализация

Основной метод вычисления интеграла в последовательной реализации:

```cpp
double TochilinEIntegralTrapeziumSEQ::ComputeIntegral() {
  std::size_t dimensions = lower_bounds_.size();
  std::vector<double> step_sizes(dimensions);
  for (std::size_t idx = 0; idx < dimensions; ++idx) {
    step_sizes[idx] = (upper_bounds_[idx] - lower_bounds_[idx]) / num_steps_;
  }

  int total_points = 1;
  for (std::size_t idx = 0; idx < dimensions; ++idx) {
    total_points *= (num_steps_ + 1);
  }

  double sum = 0.0;
  std::vector<double> point(dimensions);

  for (int idx = 0; idx < total_points; ++idx) {
    int temp = idx;
    double weight = 1.0;

    for (std::size_t dim = 0; dim < dimensions; ++dim) {
      int grid_idx = temp % (num_steps_ + 1);
      temp /= (num_steps_ + 1);
      point[dim] = lower_bounds_[dim] + (grid_idx * step_sizes[dim]);

      if (grid_idx == 0 || grid_idx == num_steps_) {
        weight *= 0.5;
      }
    }

    sum += weight * func_(point);
  }

  double volume = 1.0;
  for (std::size_t idx = 0; idx < dimensions; ++idx) {
    volume *= step_sizes[idx];
  }

  return sum * volume;
}
```

### 11.2. Параллельная реализация

Распределение работы между процессами в PreProcessingImpl:

```cpp
bool TochilinEIntegralTrapeziumMPI::PreProcessingImpl() {
  int rank = 0;
  int size = 0;
  MPI_Comm_rank(MPI_COMM_WORLD, &rank);
  MPI_Comm_size(MPI_COMM_WORLD, &size);

  int dimensions = 0;
  if (rank == 0) {
    const auto &input = GetInput();
    lower_bounds_ = input.lower_bounds;
    upper_bounds_ = input.upper_bounds;
    num_steps_ = input.num_steps;
    func_ = input.func;
    dimensions = static_cast<int>(lower_bounds_.size());
  }

  MPI_Bcast(&dimensions, 1, MPI_INT, 0, MPI_COMM_WORLD);
  MPI_Bcast(&num_steps_, 1, MPI_INT, 0, MPI_COMM_WORLD);

  if (rank != 0) {
    lower_bounds_.resize(dimensions);
    upper_bounds_.resize(dimensions);
    func_ = GetInput().func;
  }

  MPI_Bcast(lower_bounds_.data(), dimensions, MPI_DOUBLE, 0, MPI_COMM_WORLD);
  MPI_Bcast(upper_bounds_.data(), dimensions, MPI_DOUBLE, 0, MPI_COMM_WORLD);

  int total_points = 1;
  for (int idx = 0; idx < dimensions; ++idx) {
    total_points *= (num_steps_ + 1);
  }

  int base_points_per_proc = total_points / size;
  int remainder = total_points % size;

  std::vector<int> counts(size);
  std::vector<int> displs(size);
  int offset = 0;
  for (int idx = 0; idx < size; ++idx) {
    counts[idx] = base_points_per_proc + (idx < remainder ? 1 : 0);
    displs[idx] = offset;
    offset += counts[idx];
  }

  if (rank == 0) {
    for (int dest = 1; dest < size; ++dest) {
      MPI_Send(&counts[dest], 1, MPI_INT, dest, 0, MPI_COMM_WORLD);
      MPI_Send(&displs[dest], 1, MPI_INT, dest, 1, MPI_COMM_WORLD);
    }
    start_idx_ = displs[0];
    end_idx_ = displs[0] + counts[0];
  } else {
    int local_count = 0;
    int local_displ = 0;
    MPI_Recv(&local_count, 1, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
    MPI_Recv(&local_displ, 1, MPI_INT, 0, 1, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
    start_idx_ = local_displ;
    end_idx_ = local_displ + local_count;
  }

  result_ = 0.0;
  return true;
}
```

Вычисление частичного интеграла и сбор результатов в RunImpl:

```cpp
bool TochilinEIntegralTrapeziumMPI::RunImpl() {
  double local_result = ComputePartialIntegral(start_idx_, end_idx_);

  double global_result = 0.0;
  MPI_Reduce(&local_result, &global_result, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);

  int rank = 0;
  MPI_Comm_rank(MPI_COMM_WORLD, &rank);
  if (rank == 0) {
    result_ = global_result;
  }

  return true;
}
```

