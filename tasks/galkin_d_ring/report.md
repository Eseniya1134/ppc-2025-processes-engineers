# Кольцо

**Студент:** Галкин Данила Алексеевич, группа 3823Б1ПР1  
**Технология:** SEQ + MPI  
**Вариант:** 7  

---

## 1. Введение

Передача данных по виртуальным топологиям — отдельная важная тема в MPI.  
Кольцо (ring) — одна из самых простых топологий: у каждого процесса есть два соседа, “левый” и “правый”.

В этой работе нужно реализовать передачу данных от произвольного процесса-источника (`src`) к процессу-получателю (`dest`) по кольцу:

- в **MPI-версии** топологию нужно задать вручную, без `MPI_Cart_create` и `MPI_Graph_create`.

---

## 2. Постановка задачи

**Входные данные**:

```cpp
struct InType {
  int src;    // процесс-источник
  int dest;   // процесс-получатель
  int count;  // размер передаваемого массива
};
````

**Выход:** одно число `int`:

* `1` — данные успешно дошли от `src` до `dest`;
* `0` — ошибка.

Ограничения:

* `0 ≤ src, dest < size`;
* `count > 0`;
* корректная работа должна быть обеспечена при любом количестве процессов: от 1 и выше.

Передаваемые данные — массив вида:

```cpp
buffer[i] = i + 1;
```

---
## 3. SEQ реализация
**Логика работы SEQ-версии**

Последовательная версия задачи реализована в виде заглушки и выполняется в одном процессе (`rank = 0`).  
В ходе выполнения проверяется корректность входных параметров: значение `count` должно быть положительным, а процессы-источник и получатель должны совпадать (`src = dest = 0`).  
При выполнении этих условий задача считается успешно завершённой, и выходное значение устанавливается равным `1`.

Таким образом, SEQ-версия используется как базовая последовательная реализация-заглушка и предназначена для проверки корректности интеграции задачи в общий фреймворк.

---

## 4. Параллельная реализация (MPI)
MPI-версия реализует реальный алгоритм передачи данных по кольцу.

В отличие от SEQ-версии, здесь выполняется фактическая передача массива между процессами.
Кольцевая топология формируется вручную, без использования MPI_Cart_create и MPI_Graph_create.

**Логика алгоритма**
Передача данных осуществляется по часовой стрелке от процесса-источника src к процессу-получателю dest.
Число шагов передачи определяется как:
`steps = (dest - src + size) % size;`

Далее выполняется последовательная передача данных по кольцу.
На каждом шаге step явно вычисляются процесс-отправитель и процесс-получатель:
```
sender   = (src + step - 1) % size;
receiver = (src + step) % size;
```

Каждый процесс участвует в передаче не более одного раза:
- если текущий процесс является sender, он отправляет буфер следующему процессу;
- если текущий процесс является receiver, он принимает буфер от предыдущего процесса;
- остальные процессы в данном шаге не участвуют.

Передача выполняется с использованием MPI_Send и MPI_Recv в общем коммуникаторе.

**Проверка корректности**
После завершения передачи процесс dest проверяет корректность полученных данных.
Ожидается, что массив имеет вид:
`buffer[i] = i + 1`

Результат проверки агрегируется на всех процессах с помощью коллективной операции:
```cpp
MPI_Allreduce(..., MPI_LAND, ...)
```
Если данные доставлены корректно, итоговое значение GetOutput() на всех процессах равно 1, в противном случае — 0.

## 6. Экспериментальная часть

### Аппаратная платформа

Запуск выполнялся в DevContainer (Docker).
Хост-машина:

* MacBook Pro 14"
* Apple M4 Pro (12 ядер)
* 24 GB RAM

### ПО в контейнере

* Linux (Ubuntu)
* OpenMPI
* GCC / Clang
* Сборка в режиме `Release` (через CMake)

### Параметры экспериментов

* размер сообщения: `count = 5 000 000`;
* число процессов: `2`, `4`, `8`, `10`.

---

## 7. Результаты
### 7.1 Проверка корректности
Версия MPI:
* проходит все функциональные тесты;
* корректно обрабатывают неверный ввод;
* дает одинаковый результат на всех процессах;
* успешно повторно выполняют одну и ту же задачу.
Получатель всегда получает массив `[1, 2, …, count]`, поэтому итоговый `GetOutput()` на всех рангах равен `1`.

---

### 7.2 Производительность

В таблице приведены времена выполнения.
В качестве базового варианта для расчёта ускорения использовалась MPI-версия при 2 процессах.

| Режим | Count      | Время, s | Ускорение | Эффективность | Процессы |
| ---- | ---------- | -------: | --------: | ------------: | -------: |
| mpi  | 5 000 000  | 0.00336  | 1.00      | 0.50          | 2 |
| mpi  | 5 000 000  | 0.00429  | 0.78      | 0.20          | 4 |
| mpi  | 5 000 000  | 0.00691  | 0.49      | 0.06          | 8 |
| mpi  | 5 000 000  | 0.01384  | 0.24      | 0.02          | 10 |

**Комментарии по результатам:**
* При увеличении количества процессов путь по кольцу растёт → время растёт.
* Ускорения не наблюдается и не должно наблюдаться — это нормальное поведение.
* Так как в задаче нет вычислений, а только передача данных, больших ускорений здесь не появляется — всё уходит в коммуникации.
## 8. Заключение
В рамках работы реализован алгоритм передачи данных по кольцу:
* MPI-версия реализует кольцо вручную — без MPI_Cart_create и без MPI_Graph_create.
В итоге:
* корректно передает данные между любыми двумя процессами;
* проходит все тесты;
* одинаково обрабатывают корректный и некорректный ввод;
* дают предсказуемые результаты при изменении числа процессов.
Рост времени при увеличении числа процессов объясняется тем, что сообщение проходит больший путь по кольцу — это подтверждается экспериментально.

---

## 9. Литература

1. MPI Forum — *MPI: A Message-Passing Interface Standard*
2. OpenMPI Documentation
3. Google Test Documentation 
4. Сысоев А. В. Лекции по параллельному программированию. — Н. Новгород: ННГУ, 2025.