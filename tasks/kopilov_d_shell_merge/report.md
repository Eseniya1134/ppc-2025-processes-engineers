# Реализация параллельной сортировки Шелла с использованием MPI


- **Студент:** Копылов Данила Алексеевич
- **Группа:** 3823Б1ПР5
- **Технология:** MPI
- **Вариант:** Сортировка Шелла с простым слиянием

## 1. Введение
Целью данной работы является реализация параллельного алгоритма сортировки числового массива с использованием технологии MPI. Выбранный подход основывается на комбинации сортировки Шелла для локальной обработки данных на каждом процессе и последующем последовательном слиянии отсортированных фрагментов на корневом процессе.

## 2. Постановка задачи
Задача состоит в сортировке по возрастанию одномерного массива (вектора) целых чисел. Исходный неотсортированный массив находится на корневом процессе (ранг 0). После выполнения параллельного алгоритма отсортированный массив должен быть доступен на всех процессах.

Для решения задачи входные и выходные данные представлены в виде векторов:
```cpp
using InType = std::vector<int>;
using OutType = std::vector<int>;
```

## 3. Описание алгоритма
### 3.1. Последовательный алгоритм
Последовательная версия, реализованная в классе `KopilovDShellMergeSEQ`, не является классической сортировкой Шелла всего массива. Вместо этого она имитирует структуру параллельного подхода для двух частей:
1.  Исходный массив делится на две равные (или почти равные) половины.
2.  Каждая половина сортируется отдельно с помощью **сортировки Шелла**.
3.  Два отсортированных подмассива сливаются в один итоговый массив с помощью стандартной процедуры слияния.

Этот вариант служит для отладки и проверки корректности логики слияния.

### 3.2. Параллельный алгоритм (Схема распараллеливания)
Параллельный алгоритм реализует стратегию "разделяй, сортируй и властвуй":

1.  **Подготовка (Pre-processing)**:
    - Процесс с рангом 0, владеющий исходным массивом, вычисляет, как разделить массив на `N` частей, где `N` — общее число процессов.
    - С помощью `MPI_Scatterv` данные распределяются между всеми процессами. Каждый процесс получает свой фрагмент (подмассив).
2.  **Локальная сортировка (Run)**:
    - Каждый процесс, включая корневой, **независимо и параллельно** применяет **сортировку Шелла** к своему локальному фрагменту данных.
3.  **Сбор и слияние (Post-processing)**:
    - Процессы с рангом от 1 до `N-1` отправляют свои отсортированные локальные массивы на корневой процесс (ранг 0).
    - Процесс 0 последовательно принимает отсортированные фрагменты от других процессов и сливает их со своим отсортированным фрагментом. Этот этап является **узким местом**, так как вся работа по слиянию выполняется на одном процессе.
4.  **Синхронизация результата**:
    - После завершения слияния процесс 0 содержит полностью отсортированный глобальный массив.
    - С помощью широковещательной рассылки `MPI_Bcast` итоговый массив распространяется на все остальные процессы для завершения задачи.

## 4. Детали реализации
- **`KopilovDShellMergeSEQ`**: Последовательная реализация, описанная выше.
- **`KopilovDShellMergeMPI`**: Параллельная реализация.
  - `PreProcessingImpl`: Распределяет данные с помощью `MPI_Scatterv`.
  - `RunImpl`: Вызывает `ShellSort` для локального вектора `local_`.
  - `PostProcessingImpl`: Организует сбор данных на корневом процессе и последующее слияние. Другие процессы отправляют свои данные, корневой — принимает и сливает. В конце результат рассылается всем через `MPI_Bcast`.

## 5. Тестирование
Для проверки корректности и производительности были разработаны автоматизированные тесты на базе Google Test.

- **Функциональные тесты** проверяют правильность работы алгоритма для MPI и последовательной версий. Тесты генерируют случайный массив, выполняют сортировку и сравнивают итоговый результат с результатом, полученным с помощью `std::sort`. Это гарантирует корректность реализации.
- **Тесты производительности** измеряют время выполнения всей задачи для различного числа процессов, что позволяет проанализировать масштабируемость алгоритма.

## 6. Экспериментальные результаты

### 6.1. Окружение
- **Процессор:** Intel(R) Core(TM) i7-10700K CPU @ 3.80GHz
- **Операционная система:** Windows 10
- **Компилятор:** MSVC 19.38.33134.0
- **Тип сборки:** Release
- **Библиотека MPI:** Microsoft MPI (MS-MPI v10.1.2)
- **Размер массива:** 200,000 элементов

### 6.2. Производительность
Для анализа производительности измерялось время выполнения MPI-программы с разным числом процессов. Время последовательного выполнения (на 1 процессе) составило `0.090 с`.

| Режим | Количество процессов | Время, с | Ускорение | Эффективность |
|-------|----------------------|----------|-----------|---------------|
| mpi   | 2                    | 0.052    | 1.73      | 0.87          |
| mpi   | 4                    | 0.051    | 1.76      | 0.44          |
| mpi   | 8                    | 0.059    | 1.52      | 0.19          |
| mpi   | 16                   | 0.124    | 0.72      | 0.05          |


**Анализ**:
Результаты показывают, что ускорение достигается при 2 и 4 процессах. Однако при дальнейшем увеличении числа процессов (8 и 16) время выполнения начинает расти. Это указывает на то, что для данного размера задачи (200 000 элементов) накладные расходы на коммуникацию и последовательное слияние на корневом процессе становятся доминирующими и превышают выгоду от параллельной сортировки локальных данных. Эффективность резко падает с увеличением числа процессов, что подтверждает наличие узкого места в алгоритме.

При 16 процессах наблюдается замедление (ускорение < 1), что говорит о нецелесообразности использования такого количества процессов для данного размера задачи в этой реализации.

## 7. Выводы
В ходе работы был успешно реализован и протестирован параллельный алгоритм сортировки на основе сортировки Шелла и последующего слияния. Анализ производительности показал, что алгоритм дает ускорение, однако его масштабируемость ограничена последовательным характером этапа слияния на корневом процессе. Для дальнейшего улучшения производительности следовало бы реализовать параллельный алгоритм слияния (например, слияние попарно или с использованием дерева).

## 8. Источники
1.  Документация MPI (Open MPI / MS-MPI).
2.  Документация C++ Standard Library.
3.  Официальная документация Google Test.
