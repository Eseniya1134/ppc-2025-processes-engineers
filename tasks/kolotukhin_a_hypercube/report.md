# Топологии сетей передачи данных. Гиперкуб
- Студент: Колотухин Александр Дмитриевич, 3823Б1ПР2
- Технология: SEQ, MPI
- Вариант: 10

## 1. Введение
Рассматривается задача реализации виртуальной топологии гиперкуб для передачи данных между произвольными процессами в MPI-программе. Цель работы — смделировать топологию гиперкуба для передачи данных между произвольными процессами, используя только базовые операции обмена (`MPI_Send` и `MPI_Recv`) без встроенных механизмов `MPI_Graph_create` или `MPI_Cart_create`.

## 2. Постановка задачи
Реализовать MPI-программу, моделирующую передачу данных в топологии гиперкуба.

**Входные данные (`InType`):**

Информация из сообщения `HypercubeMessage`, которое представлено целочисленным вектором из 3 элементов, передаваемое между процессами со следующей структурой:
- элемент `[0](source)` — процесс-отправитель
- элемент `[1](dest)` — процесс-получатель
- элемент `[2](data_size)` — количество элементов для генерации и передачи между процессами

**Выходные данные (`OutType`):**
- `data` (целое число) — сумма элементов вектора, доставленного в процесс-получатель

**Ограничения:**
- Количество процессов `world_size` может не быть степенью двойки
- Номера процессов `source` и `dest` должны быть в допустимом диапазоне: `0 <= source < world_size` и  `0 <= dest < world_size`

## 3. Базовый алгоритм (вычисление пути)
### Структура и нумерация процессов 3-мерного гиперкуба
```text
      110───────111
      ╱│        ╱│
     ╱ │       ╱ │
   010─┼─────011 │
    │ 100─────│─101
    │ ╱       │ ╱
   000───────001
```
Каждый процесс соседствует с `n` процессами, отличными от исходного на один бит в двоичном представлении номеров. 

**Алгоритм**:
1. Вычисление размерности гиперкуба по числу процессов
2. Определение кратчайшего пути от source к dest через XOR битов
3. Последовательная передача данных по найденному пути
4. процесс-получатель передает всем процессам полученные данные.

```cpp
std::vector<int> KolotukhinAHypercubeMPI::CalcPath(int source, int dest, int dimensions) {
  std::vector<int> path;
  int current = source;
  path.push_back(current);
  int xor_val = source ^ dest;
  // Если source > dest, начинаем со старших битов, иначе - с младших
  if (source > dest) {
    for (int dim = dimensions - 1; dim >= 0; dim--) {
      int mask = 1 << dim;
      if ((xor_val & mask) != 0) {
        current = current ^ mask;
        path.push_back(current);
        if (current == dest) {
          break; 
        }
      }
    }
  } else {
    for (int dim = 0; dim < dimensions; dim++) {
      int mask = 1 << dim;
      if ((xor_val & mask) != 0) {
        current = current ^ mask;
        path.push_back(current);
        if (current == dest) {
          break;
        }
      }
    }
  }
  return path;
}
```

## 4. Схема распараллеливания
**MPI-версия (без встроенных топологий MPI):**

**Декомпозиция**: 
- Каждый процесс представляет один узел гиперкуба.

**Роли процессов:**
- **Источник (`source`)**: генерирует вектор данных заданного размера (элементы `1`) и начинает передачу по вычисленному пути
- **Промежуточные узлы  (процессы на пути, исключая `source` и `dest`)**: получают данные, пересылают следующему узлу
- **Получатель (`dest`)**: принимает финальные данные, обрабатывает их, рассылает результат обработки всем процессам
- **Остальные процессы**: примут данные обработанные финальные данные, когда они будут у процесса-получателя

**Коммуникация:**
- Каждый процесс независимо вычисляет кратчайший путь от `source` к `dest`, используя только их ранги и общее количество процессов
- Процесс определяет свое положение на пути (предыдущий и следующий сосед)
- Передача данных осуществляется строго по этому пути с использованием парных операций `MPI_Send` и `MPI_Recv` в глобальном коммуникаторе `MPI_COMM_WORLD`


## 5. Детали реализации
Структура кода

```text
kolotukhin_a_hypercube   
    │
    ├───common
    │   └───include
    │       └───common.hpp — Общие типы данных
    ├───mpi
    │   ├───include
    │   │   └───ops_mpi.hpp — Заголовочный файл параллелльной версии программы
    │   └───src
    │       └───ops_mpi.cpp — Реализация параллельной версии
    ├───seq
    │   ├───include
    │   │   └───ops_seq.hpp — Заголовочный файл последовательной версии программы
    │   └───src
    │       └───ops_seq.cpp — реализация последовательной версии
    └───tests
        ├───functional
        │   └───main.cpp — Тесты оценки функционирования
        └───performance
            └───main.cpp — Тесты оценки производительности
```

**Ключевые методы класса `KolotukhinAHypercubeMPI`**
- `CalculateHypercubeDimension(int world_size)`: вычисление размерности гиперкуба
- `GetNeighbor(int rank, int dimension)`: вычисляет ранг соседа в измерении
- `PerformComputeLoad(int iterations)`: имитация вычислительной нагрузки 
- `RunImpl()`: основная логика, вычисление пути и передачу данных

Для имитации реальной вычислительной нагрузки каждый узел выполняет 150000 итераций вычислений, что позволяет оценить влияние вычислений на общее время работы.

## 6. Экспериментальная установка
- Аппаратное обеспечение / ОС 
  - Модель процессора: Intel Core 7 240H (10 ядер, 16 потоков)
  - Оперативная память: 16 ГБ
  - Версия ОС: Windows 11 Home 25H2 (26200.6899)
- Инструменты 
  - Компилятор: MSVC версии 19.44.35217.0 (Visual Studio 2022)
  - Тип сборки: Release
  - MPI: Microsoft MPI 10.1.12498.52
- Окружение
  - Количество процессов: 2, 4, 8, 16, 32
- Данные
  - Тестовые данные генерируются внутри процесса-источника (`400` элемнтов), в тестах производительности передача осуществляется от `0` процесса к последнему

## 7. Результаты и обсуждение
### 7.1 Корректность
Корректность работы проверялась набором функциональных тестов (Google Test). Основные сценарии:
- Передача между соседними процессами (0 -> 1)
- Передача между удаленными процессами (0 -> 3)
- Передача от ненулевого процесса (2 -> 3)
- Передача от процесса самому себе (2 -> 2)
- Передача пустого массива (0 -> 3)
- Передача в обратном направлении (3 -> 0)

Во всех тестах проверялось, что все процессы получили обработанные исходные данные от процесса-получателя — сумму элементов.

### 7.2 Производительность
Результаты performance-тестов (режим `task_run`) для MPI-версии:

#### MPI-версия:
| Число Процессов | Время выполнения (с) |   Шаги передачи    |
|:---------------:|:--------------------:|:------------------:|
|        2        |        0.00153       |         1          |
|        4        |	       0.00158       |         2          |
|        8        |	       0.00163       |         3          |
|       16        |	       0.00165       |         4          |
|       32        |     	 0.00236       |         5          |

#### Анализ: 
- Время выполнения для 2, 4, 8, 16 и 32 процессов остается стабильным, демонстрируя низкие накладные расходы
- Незначительный рост времени с увеличением количества процессов объясняется увеличением длины пути передачи и числа операций `MPI_Send` и `MPI_Recv` 
- Основное время затрачивается на имитацию вычислительной нагрузки `PerformComputeLoad()`

## 8. Выводы
**Основные результаты:**
- Реализация корректно выполняет задачу по передаче данных по гиперкубу
- Функциональные тесты подтверждают корректность работы реализации
- MPI-версия использует не использует встроенные механизмы `MPI_Graph_create` или `MPI_Cart_create`
- MPI-версия демонстрирует хорошую производительность за счет малых накладных расходов

**Практическая значимость**: работа демонстрирует, что для специализированных топологий, таких как гиперкуб, ручная эмуляция может быть эффективной, обеспечивающей контроль над коммуникацией и минимальные накладные расходы.

## 9. Источники
1. Документация по курсу «Параллельное программирование» / Parallel Programming Course [Электронный ресурс]. — Режим доступа: https://learning-process.github.io/parallel_programming_course/ru/index.html. — Дата обращения: 14.12.2025.
2. Kendall W. MPI Tutorial [Электронный ресурс]. — URL: https://mpitutorial.com. — Дата обращения: 13.12.2025.
3. Сысоев А. В. Курс лекций по параллельному программированию [Электронный ресурс]. — URL: https://source.unn.ru. — Требуется авторизация. — Дата обращения: 14.12.2025.

## Приложение
```cpp
bool KolotukhinAHypercubeMPI::RunImpl() {
  int rank = 0;
  int world_size = 0;
  MPI_Comm_rank(MPI_COMM_WORLD, &rank);
  MPI_Comm_size(MPI_COMM_WORLD, &world_size);

  const auto &input = GetInput();
  int source = input[0];
  int dest = input[1];

  std::vector<int> data{};
  int data_size = 0;

  int dimensions = 0;
  dimensions = CalculateHypercubeDimension(world_size);
  if (rank == source) {
    data_size = input[2];
    data.resize(static_cast<size_t>(data_size), 1);
  }

  if (source == dest) {
    MPI_Bcast(&data_size, 1, MPI_INT, dest, MPI_COMM_WORLD);
    if (rank != dest) {
      data.resize(static_cast<size_t>(data_size));
    }
    MPI_Bcast(data.data(), data_size, MPI_INT, dest, MPI_COMM_WORLD);
    GetOutput() = std::accumulate(data.begin(), data.end(), 0);
    return true;
  }

  std::vector<int> path = CalcPath(source, dest, dimensions);

  int my_position = -1;
  int prev_neighbor = -1;
  int next_neighbor = -1;
  CalcPositions(rank, path, my_position, next_neighbor, prev_neighbor);
  if (my_position != -1) {
    if (rank == source) {
      PerformComputeLoad(150000);
      SendData(data, next_neighbor);
    } else if (rank == dest) {
      RecvData(data, prev_neighbor);
      data_size = static_cast<int>(data.size());
      PerformComputeLoad(150000);
    } else {
      RecvData(data, prev_neighbor);
      data_size = static_cast<int>(data.size());
      PerformComputeLoad(150000);
      SendData(data, next_neighbor);
    }
  }

  MPI_Bcast(&data_size, 1, MPI_INT, dest, MPI_COMM_WORLD);

  if (my_position == -1) {
    data.resize(static_cast<size_t>(data_size));
  }

  MPI_Bcast(data.data(), data_size, MPI_INT, dest, MPI_COMM_WORLD);

  GetOutput() = std::accumulate(data.begin(), data.end(), 0);
  MPI_Barrier(MPI_COMM_WORLD);
  return true;
}
```