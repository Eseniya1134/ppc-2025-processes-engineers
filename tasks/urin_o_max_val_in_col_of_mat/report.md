# Поиск максимальных значений в столбцах матрицы  
* Студент: Юрин Олег Игоревич., группа 3823Б1ПР4
* Технология: SEQ | MPI  
* Вариант: 16

## 1. Введение
Задача заключается в поиске максимального значения в каждом столбце квадратной матрицы. Это распространённая операция в линейной алгебре и анализе данных, где требуются покоординатные статистики. Параллельная реализация с использованием MPI позволяет эффективно обрабатывать большие матрицы путем распределения столбцов между несколькими процессами.


## 2. Постановка задачи
Для квадратной матрицы размером **n × n** необходимо найти максимальное значение в каждом столбце и вернуть вектор этих значений.


* Вход: целое число *n* (размер матрицы)  
* Выход: вектор из *n* целых чисел с максимальными значениями каждого столбца  
* Ограничения: *1 ≤ n ≤ 10000*


## 3. Базовый алгоритм (последовательный)

```cpp

для каждого столбца j от 0 до n-1:
    max_val = matrix[0][j]
    для каждой строки i от 1 до n-1:
        если matrix[i][j] > max_val:
            max_val = matrix[i][j]
    column_maxes[j] = max_val
```

## 4. Схема распараллеливания (MPI)
**Распределение данных**

* Столбцы распределяются между процессами (циклически).

* Каждый процесс находит максимумы в своих столбцах.

* Корневой процесс собирает результаты и отправляет всем полный вектор максимумов.

**Шаблон коммуникации**

* Распределение столбцов

* Локальные вычисления

* Сбор результатов (MPI_Gatherv)

* Рассылка результатов (MPI_Bcast)

**Псевдокод**
```
MPI_Comm_rank, MPI_Comm_size
Вычисление local_columns = n / size + остаток
для local_col в local_columns:
    найти максимум столбца
MPI_Gatherv локальных результатов в корневой процесс
MPI_Bcast полного вектора результатов всем
```

## 5. Детали реализации

**Структура кода:**

* `ops_mpi.hpp` - MPI реализация

* `ops_seq.hpp` - Последовательная реализация

* `common.hpp` - Определения типов и общие утилиты

* Функциональные тесты в `tests/functional/`

* Тесты производительности в `tests/performance/`

**Основные классы:**

* `UrinOMaxValInColOfMatMPI` - Параллельная MPI реализация

* `UrinOMaxValInColOfMatSeq` - Последовательная реализация

**Использование памяти:**

* Каждый процесс хранит всю матрицу (можно оптимизировать)

* Размер выходного вектора: n целых чисел

## 6. Экспериментальная установка

**Аппаратное обеспечение/ОС:**

* Процессор: 12th Gen Intel(R) Core(TM) i5-12450H   2.00 GHz

* ОЗУ: 16 ГБ

* ОС: Windows

**Инструментарий:**

* Компилятор: MSVC

* MPI: MS-MPI

* Build Type: Release

**Окружение:**

* PPC_NUM_THREADS: 4

* PPC_NUM_PROC: 4

* Размер матрицы для тестов производительности: 10000×10000

## 7. Результаты и обсуждение

### 7.1 Корректность

**Корректность проверена через:**

* Модульные тесты для различных размеров матриц (1×1 до 10×10)

* Сравнение между последовательными и MPI результатами

* Проверка размера выходного вектора и положительных значений

* Все 20 функциональных тестов успешно пройдены

* Проверка диапазона значений (1-1000) в тестах производительности

### 7.2 Производительность

**Результаты тестов производительности для матрицы 10000×10000:**

| Процессы | Режим     | Версия  | Время, сек | Ускорение | Эффективность |
| -------- | --------- | ------- | -----------| --------- | ------------- |
| 1        | task_run  | SEQ     | 0.56375    | 1.00      | N/A           |
| 1        | task_run  | MPI     | 0.80188    | 0.70      | 70.0%         |
| 1        | pipeline  | MPI     | 3.98500    | 0.14      | 14.1%         |
| 2        | task_run  | MPI     | 0.92228    | 0.61      | 30.5%         |
| 2        | pipeline  | MPI     | 4.64900    | 0.12      | 6.1%          |
| 4        | task_run  | MPI     | 1.16673    | 0.48      | 12.1%         |
| 4        | pipeline  | MPI     | 6.83700    | 0.08      | 2.1%          |

**Пояснения:**

* SEQ время (0.56375 сек) используется как базовое для расчета ускорения

* Ускорение = SEQ_время / Текущее_время

* Эффективность = Ускорение / Количество_процессов × 100%

* Время pipeline переведено из миллисекунд в секунды (2843 мс = 2.843 сек, 4649 мс = 4.649 сек, 6837 мс = 6.837 сек, 3985 мс = 3.985 сек)

## Выводы

### Общая картина

* MPI реализации показывают худшую производительность по сравнению с последовательной версией

* С ростом числа процессов производительность ухудшается, что противоречит ожиданиям от параллельных вычислений

### Производительность по режимам

**Task_run режим**

* Лучший результат: 1 процесс MPI (70% эффективности)

* С ростом процессов падает: 2 процесса → 30.5%, 4 процесса → 12.1%

* Время выполнения растет: 0.80с → 0.92с → 1.17с

**Pipeline режим**

* Крайне низкая производительность: эффективность 14.1% → 6.1% → 2.1%

* Время выполнения значительно выше: 3.99с → 4.65с → 6.84с

* Наихудший вариант среди всех тестируемых конфигураций

### Критические проблемы**

* Отрицательное масштабирование - добавление процессов увеличивает время выполнения

* Низкая эффективность - даже лучший результат всего 70% от SEQ версии

* Накладные расходы MPI значительно превосходят выгоду от распараллеливания

### Рекомендации

* Оптимизация коммуникаций - вероятно, слишком частый обмен сообщениями

* Пересмотр алгоритма распределения - неправильное распределение нагрузки

* Pipeline режим требует полного пересмотра архитектуры

### Лучшая конфигурация

* 1 процесс MPI в task_run режиме (0.80с, 70% эффективности)

* SEQ версия остается абсолютным лидером (0.56с)

## 9. Ссылки

1. Документация MPI: https://www.open-mpi.org/

2. Документация Google Test: https://google.github.io/googletest/

3. Руководство по параллельному программированию

## Приложение: Ключевой фрагмент MPI реализации

```cpp

std::vector<int> local_maxes(local_cols_count, 0);
for (int local_idx = 0; local_idx < local_cols_count; ++local_idx) {
    int global_col = start_col + local_idx;
    int max_val = matrix[0][global_col];
    for (int row = 1; row < n; ++row) {
        if (matrix[row][global_col] > max_val) {
            max_val = matrix[row][global_col];
        }
    }
    local_maxes[local_idx] = max_val;
}```