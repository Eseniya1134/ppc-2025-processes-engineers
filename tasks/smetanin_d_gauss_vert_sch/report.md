# Метод Гаусса — ленточная вертикальная схема Сметанин Дмитрий

- Студент: Сметанин Дмитрий Владимирович, 3823Б1ПР3
- Технологии: SEQ | MPI
- Вариант: 16

## 1. Введение
В этой работе реализованы два варианта решения задачи решения системы линейных уравнений с расширенной матрицей в ленточной форме: последовательный (`SEQ`) и параллельный (`MPI`). Цель — реализовать алгоритм, проверить корректность (функциональные тесты) и провести замеры производительности для разных чисел процессов.

## 2. Постановка задачи
Дана система линейных уравнений размера `n` с ленточной матрицей и правой частью (augmented matrix). Требуется найти решение методом Гаусса с выбором опорного элемента по столбцу, при этом учитывать ленточную структуру (ширина полосы `bandwidth`) для ограничения операций.

Вход: `GaussBandInput` (поля `n`, `bandwidth`, `augmented_matrix`).
Выход: вектор неизвестных `OutType` длины `n`.

## 3. Базовый алгоритм (последовательный)
Обычный метод Гаусса (с выбором опорного элемента) над расширенной матрицей, но с ограничением операций по диапазону столбцов и строк, определяемых полосой (`bandwidth`). Алгоритм выполняет:
- поиск опорного элемента в пределах полосы;
- обмен строк (в пределах полосы);
- вычитание строк ниже текущей (с учётом полосы);
- обратную подстановку (back substitution) с учётом полосы.

Сложность в худшем случае близка к O(n * bandwidth). Для узкой полосы это значительно дешевле, чем полная матрица.

Ограничения алгоритма: метод Гаусса с частичным выбором опорного элемента (в пределах полосы) не гарантирует полной устойчивости, как полный выбор по всей матрице. Алгоритм подходит только для ленточных матриц с заданной шириной полосы; для плотных матриц или матриц с широкой полосой эффективность снижается. Также, предполагается, что матрица невырожденная в пределах полосы для каждого столбца.

## 4. Схема распараллеливания
Параллельная версия реализована с использованием вертикальной схемы распределения данных (по столбцам), где расширенная матрица делится по столбцам между процессами. Каждый процесс хранит все строки для своих локальных столбцов для эффективного доступа. Это позволяет всем процессам участвовать в вычислениях, распределяя нагрузку и минимизируя коммуникации благодаря ленточной структуре.

Распределение данных: Общее количество столбцов (n + 1) делится на процессы с использованием блочного распределения. Каждый процесс вычисляет свой start_col и local_cols, извлекает локальную подматрицу из входных данных.
Прямой ход (forward elimination):
- Для каждого столбца i владелец находит максимальный pivot в пределах полосы, транслирует max_row и max_val всем процессам.
- Если требуется обмен строк, все процессы выполняют swap локально.
- Владелец вычисляет факторы исключения и транслирует их.
- Все процессы обновляют свои локальные строки ниже pivot, ограничиваясь столбцами в пределах полосы.

Обратный ход (back substitution):
Для каждого i от n-1 до 0:
- Каждый процесс вычисляет частичную сумму для своих столбцов в пределах полосы.
- Суммирование частичных сумм через MPI_Allreduce для получения глобальной суммы.
- Трансляция правой части от владельца augmented столбца и диагонали от владельца диагонального столбца.
- Вычисление значения решения sol[i].

## 5. Детали реализации
tasks\smetanin_d_gauss_vert_sch\
├── common
│ └── include
│ └── common.hpp
├── info.json
├── mpi
│   ├── include
│   │   └── ops_mpi.hpp
│   └── src
│       └── ops_mpi.cpp
├── report.md
├── seq
│   ├── include
│   │   └── ops_seq.hpp
│   └── src
│       └── ops_seq.cpp
├── settings.json
└── tests
    ├── functional
    │   └── functional.cpp
    └── performance
        └── performance.cpp
- `tasks/smetanin_d_gauss_vert_sch/common/include/common.hpp` — общие типы/константы.
- `tasks/smetanin_d_gauss_vert_sch/seq/include/ops_seq.hpp` и `seq/src/ops_seq.cpp` — последовательная реализация.
- `tasks/smetanin_d_gauss_vert_sch/mpi/include/ops_mpi.hpp` и `mpi/src/ops_mpi.cpp` — MPI-реализация с вертикальной схемой.
- `tasks/smetanin_d_gauss_vert_sch/tests/functional/main.cpp` — функциональные тесты.
- `tasks/smetanin_d_gauss_vert_sch/tests/performance/main.cpp` — перфоманс тесты (pipeline/task_run).

## 6. Экспериментальная установка
Hardware/OS: AMD Ryzen 5 5600H, Cores/Threads: 6/12, 16GB RAM, Windows 11 x64.
Toolchain: Microsoft Visual C++, Visual Studio Code 2019/2022, Release, Microsoft MPI 10.1.
Environment: mpiexec -n N, MPI_COMM_WORLD.

## 7. Результаты и обсуждение

Функциональные тесты (запуск `mpiexec -n 4 .\build\bin\ppc_func_tests.exe`):
- Всего выполнено 6 тестов — все пройдены: все тесты `GaussBandTests` прошли успешно.

Перфоманс тесты (шесть запусков: mpiexec -n 1,2,4,7,8). В таблицах "Count" — число MPI-процессов при запуске (P).

Pipeline mode (время, скорость):

|      Mode      | P |  Time (s)  |       Speedup (seq/mpi)       | Efficiency (%) |
|----------------|--:|-----------:|------------------------------:|---------------:|
| pipeline (mpi) | 1 | 0.01621680 |             0.95x             | 95%            |
| pipeline (mpi) | 2 | 0.00853504 |             1.81x             | 90.5%          |
| pipeline (mpi) | 4 | 0.00486800 |             3.21x             | 80.3%          |
| pipeline (mpi) | 7 | 0.00318806 |             4.97x             | 71%            |
| pipeline (mpi) | 8 | 0.00273568 |             5.70x             | 71.3%          |

Task run mode (время, скорость):
|      Mode      | P |  Time (s)  |       Speedup (seq/mpi)       | Efficiency (%) |
|----------------|--:|-----------:|------------------------------:|---------------:|
| task_run (mpi) | 1 | 0.01586098 |             0.97x             | 97%            |
| task_run (mpi) | 2 | 0.00823440 |             1.87x             | 93.5%          |
| task_run (mpi) | 4 | 0.00458866 |             3.42x             | 85.5%          |
| task_run (mpi) | 7 | 0.00306084 |             5.13x             | 73.3%          |
| task_run (mpi) | 8 | 0.00266608 |             5.92x             | 74%            |

Примечания по таблицам:
- `Speedup` рассчитан как отношение времени последовательной версии (`seq`) к времени `mpi`.
- `Efficiency` = Speedup / P * 100%.

## 8. Заключение
- Для P=1 оба режима дают близкие времена (speedup ≈0.95–0.97) — значит накладные расходы MPI и логика кода сопоставимы с последовательной реализацией, с минимальным оверхедом.
- Для P=2,4,7,8: speedup от ~1.8× до ~5.9× в зависимости от режима и набора замеров. Эффективность варьируется от ~71% до ~93.5%, что говорит о том, что на выбранных входах параллелизация даёт выигрыш, но с ростом P эффективность снижается из-за накладных расходов MPI на коммуникации (broadcasts и allreduce).
- Самая высокая относительная выгода наблюдается для больших относительных различий между последовательным `seq` и `mpi`.

## 9. Ссылки
1. Материалы курса: <https://learning-process.github.io/parallel_programming_course/ru/common_information/report.html>.
2. Microsoft MPI: <https://learn.microsoft.com/ru-ru/message-passing-interface/mpi-reference>.
3. OpenMPI документация: <https://www.open-mpi.org/>.
4. Сысоев А. В. *Лекции по параллельному программированию*.