# Обобщённая передача данных от всех процессов одному (MPI Gather)

- Студент: Марьин Лев Михайлович, группа 3823Б1ПР3
- Технология: SEQ | MPI
- Вариант: 5

## 1. Введение 

- Операция gather одно из базовых операций в параллельном программировании с использованием MPI. Её назначение - собрать данные от всех процессов в один процесс.
- Цель работы - разработать корректную и масштабируемую реализацию операции gather, исследовать её корректность и производительность по сравнению с последовательным вариантом.

## 2. Постановка задачи

- Задача: Реализовать обобщённую передачу данных от всех процессов одному.
- Входные данные: массив data типа std::vector<char>, count - количество элементов на процесс, datatype  тип данных (MPI_INT, MPI_FLOAT,MPI_DOUBLE), root - номер корневого процесса.
- Выходные данные: на корневом процессе - массив, содержащий данные всех процессов, упорядоченные по рангу, на остальных процессах - пустой результат.

## 3. Базовый алгоритм (последовательный) 

- В последовательном варианте задача реализуется копирование входного буфера в выходной.

Алгоритм:
- Проверить корректность входных данных.
- Скопировать входной массив в выходной.
- Вернуть результат.

## Сложность алгоритма:
- Временная: O(n).
- Пространственная: O(n).

## 4. Схема параллелизации
- В MPI-версии используется древовидная схема сбора данных.

Основные идеи:
- процессы перенумеровываются относительно корня;
- сбор данных происходит по этапам с пронумерованным шагом;
- на каждом этапе часть процессов отправляет накопленные данные процессам-получателям;
- корневой процесс в конце получает данные от всех процессов.

## Коммуникации выполняются с помощью:
- MPI_Send
- MPI_Recv

- Для восстановления порядка данных хранится дополнительный массив рангов, соответствующих каждому блоку данных.

## 5. Детали реализации

## Структура проекта
- `common.hpp` — общие типы данных
- `ops_seq.hpp / ops_seq.cpp` — последовательная версия
- `ops_mpi.hpp / ops_mpi.cpp` — MPI версия
- `functional_tests.cpp` — функциональные тесты
- `performance_tests.cpp` — тесты производительности

## Основные классы
- `MarinLGenerTransmFrAllToOneGatherSEQ` — последовательная реализация
- `MarinLGenerTransmFrAllToOneGatherMPI` — MPI-реализация 

## Основные методы
- `ValidationImpl()` — проверка корректности входных данных
- `PreProcessingImpl()` — подготовка
- `RunImpl()` — основной алгоритм
- `PostProcessingImpl()` — завершение работы

## Особенности реализации
- Поддержка типов int, float, double
- Корректная работа при size = 1
- Корректная работа при count = 1
- Использование собственного алгоритма TreeGatherImpl
- Отсутствие вызовов MPI_Gather

## Использование памяти
- **SEQ**: O(N)
- **MPI**: O(N) на корне

## Граничные случаи
- Count <= 0 - ошибка валидации
- Некорректный datatype - ошибка
- Root >= size - ошибка
- Один процесс
- Средний корень 
- Большие объёмы данных

## 6. Экспериментальная установка

## Аппаратное обеспечение
- **Процессор**: Intel Core i5-12450H
- **ОЗУ**: 16 ГБ
- **ОС**: Windows 10 Pro

## Программное обеспечение
- **Компилятор**: MSVC 19.36
- **MPI**: MS-MPI 10.0
- **CMake**: 4.1.2
- **Фреймворк тестирования** : Google Test

## Параметры тестирования

**Разные сценарии**: 
- Сбор данных при root = 0
- Минимальный размер данных (count = 1)
- Большие объёмы данных
- Различные типы данных (int, float, double)
- Проверка корректного порядка данных в выходном буфере

**Функциональные тесты**:
- Размер блока данных от 1 до 1000 элементов
- Типы данных: MPI_INT, MPI_FLOAT, MPI_DOUBLE
- Корневой процесс: 0

## Тесты производительности
- Длина: 50000000
- Типы данных: MPI_INT, MPI_FLOAT, MPI_DOUBLE
- Количество процессов MPI: 1, 2, 3, 4
- Режимы выполнения: task_run, pipeline

## 7. Результаты и обсуждение

## 7.1 Корректность

## Все функциональные тесты успешно пройдены
Тесты учитывают: 
- Базовый gather
- Неверные входные данные
- Большие массивы
- SEQ и MPI версии


### 7.2 Производительность

Тестирование проводилось с длиной 50 000 000 

- task_run:

| Режим | Процессов | Время, с | Ускорение | Эффективность |
| ----- | --------: | -------: | --------: | ------------: |
| seq   |         1 |  0.02262 |      1.00 |           N/A |
| mpi   |         1 |  0.28752 |      0.08 |          7.9% |
| mpi   |         2 |  0.57230 |      0.04 |          2.0% |
| mpi   |         3 |  0.96400 |      0.02 |          0.8% |
| mpi   |         4 |  1.29530 |      0.02 |          0.4% |

- task_pipeline:

| Режим | Процессов | Время, с | Ускорение | Эффективность |
| ----- | --------: | -------: | --------: | ------------: |
| seq   |         1 |  0.02231 |      1.00 |           N/A |
| mpi   |         1 |  0.29649 |      0.08 |          7.5% |
| mpi   |         2 |  0.82388 |      0.03 |          1.4% |
| mpi   |         3 |  1.43997 |      0.02 |          0.5% |
| mpi   |         4 |  2.01992 |      0.01 |          0.3% |

## Анализ производительности

Расчёт метрик:
- Ускорение = T_seq / T_mpi
- Эффективность = (Speedup / P) × 100%

## Анализ результатов

**task_run**:
- На одном процессе MPI-реализация работает существенно 
медленнее последовательной версии из-за накладных расходов 
на инициализацию MPI, организацию обменов и просто версии SEQ.
- При увеличении числа процессов до 2, 3 и 4 время выполнения продолжает расти, 
что связано с превосходством коммуникационных затрат над полезной работой.

**task_pipeline**:
- На одном процессе MPI-реализация также уступает последовательной версии,
так как дополнительные этапы конвейера не компенсируют накладные расходы.
- При увеличении числа процессов наблюдается дальнейшее увеличение времени 
выполнения, что связано с ростом количества коммуникаций и синхронизаций.

## Вывод:
- Режим task_run показывает более стабильное поведение по сравнению с 
pipeline, однако не обеспечивает ускорения относительно последовательной 
реализации.
- Режим pipeline оказывается наименее эффективным для данной задачи из-за 
высоких накладных расходов на коммуникацию.
- Для операции gather, ориентированной на передачу данных к одному процессу, 
коммуникационные издержки существенно превышают выигрыш от параллелизма.

## 8. Выводы
- Реализованы последовательная и MPI-версии алгоритма обобщённой передачи 
данных от всех процессов одному.
- Корректность реализации подтверждена функциональными тестами для 
различных сценариев входных данных.
- С увеличением числа процессов накладные расходы на коммуникацию и 
синхронизацию начинают доминировать над полезными вычислениями.

## 9. Список источников информации

1. Сысоев А. В. Лекции по параллельному программированию. — Н. Новгород: ННГУ, 2025.
2. Преснухин Л. Н. Параллельное программирование с использованием стандарта OpenMP. — Москва: Издательство МГУ, 2010.
2. Parallel Programming Course Slides - https://learning-process.github.io/parallel_programming_slides/
3. Microsoft. Microsoft MPI Documentation - https://learn.microsoft.com/en-us/message-passing-interface/microsoft-mpi 

## Приложение

### Фрагменты кода

**Последовательная версия (SEQ):**

```cpp
bool MarinLGenerTransmFrAllToOneGatherSEQ::RunImpl() {
  const auto &input = GetInput();

  GetOutput() = input.data;

  return true;
}
```

**Параллельная версия (MPI):**

```cpp
bool MarinLGenerTransmFrAllToOneGatherMPI::RunImpl() {
  int rank = 0;
  int size = 1;
  MPI_Comm_rank(MPI_COMM_WORLD, &rank);
  MPI_Comm_size(MPI_COMM_WORLD, &size);

  const auto &input = GetInput();

  int type_size = GetTypeSize(input.datatype);

  std::vector<char> recv_buffer;
  if (rank == input.root) {
    recv_buffer.resize(input.count * size * type_size);
  }

  int result =
      TreeGatherImpl(input.data.data(), input.count, input.datatype, rank == input.root ? recv_buffer.data() : nullptr,
                     input.count, input.datatype, input.root, MPI_COMM_WORLD);

  if (result != MPI_SUCCESS) {
    return false;
  }

  if (rank == input.root) {
    GetOutput() = std::move(recv_buffer);
  } else {
    GetOutput() = std::vector<char>();
  }

  return true;
}
```
