# Повышение контраста полутонового изображения посредством линейной растяжки гистограммы.

- Студент: Шкенев Илья Андреевич, группа 3823Б1ПР3
- Технология: SEQ | MPI
- Вариант: 30

## 1. Введение

Повышение контраста — важная операция обработки изображений, используемая для улучшения визуального восприятия и выделения деталей. Линейная растяжка гистограммы — один из простейших и наиболее эффективных методов повышения контраста, который переназначает интенсивности пикселей так, чтобы они занимали весь доступный диапазон [0, 255].

В данной работе реализованы две версии алгоритма:
- Последовательная (SEQ)
- Параллельная (MPI)

Цель работы — создать эффективное параллельное решение и сравнить его производительность с последовательной реализацией.

## 2. Постановка задачи

Преобразование вектора интенсивностей пикселей полутонового изображения для повышения контраста методом линейной растяжки гистограммы.

**Математическая формула:** 
- `new_value = (old_value - min_value) × 255 / (max_value - min_value)`
- old_value — исходная интенсивность пикселя
- min_value — минимальная интенсивность в изображении
- max_value — максимальная интенсивность в изображении
- new_value — новая интенсивность после растяжки

**Входные данные:**
- Вектор интенсивностей пикселей — InType = std::vector<int>

**Выходные данные:**
- Преобразованный вектор интенсивностей — OutType = std::vector<int>

**Ограничения:**
- Интенсивности пикселей находятся в диапазоне [0, 255].
- Вектор не пустой.

## 3. Базовый алгоритм (последовательный)

**Последовательный алгоритм:**

```
1. Проверка корректности входных данных:
   - Вектор не должен быть пустым
   - Все значения в диапазоне [0, 255]

2. Поиск минимального и максимального значения в векторе

3. Если min == max:
     возвращаем исходный вектор без изменений
   Иначе:
     вычисляем range = max - min
     для каждого элемента:
       новый_элемент = (старый_элемент - min) × 255 / range
```

**Сложность:**
- Время: O(n), где n — количество пикселей
- Память: O(n)

## 4. Схема параллелизации

### Распределение данных

```
1. Вертикальное разбиение вектора на равные части
2. Каждый процесс получает свой блок данных через MPI_Scatterv
3. Размер блока: base_chunk = n / P + (i < remainder ? 1 : 0)
```

### Коммуникационная схема

```
Этап 1: Инициализация (Процесс 0)
   1. Определение общего размера данных
   2. Вычисление распределения send_counts и displs
   3. Рассылка размера всем процессам (MPI_Bcast)

Этап 2: Распределение данных
   1. MPI_Scatterv - распределение блоков данных

Этап 3: Параллельные вычисления
   1. Каждый процесс находит локальные min/max
   2. MPI_Allreduce для глобальных min/max (MPI_MIN, MPI_MAX)

Этап 4: Обработка данных
   1. Каждый процесс применяет линейную растяжку к своему блоку

Этап 5: Сбор результатов
   1. MPI_Gatherv - сбор обработанных данных в процесс 0
```

**Алгоритм поиска глобальных min/max**
```
1. Локальный поиск: каждый процесс находит min_local, max_local в своем блоке
2. Глобальная редукция:
   MPI_Allreduce(&min_local, &global_min, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD)
   MPI_Allreduce(&max_local, &global_max, 1, MPI_INT, MPI_MAX, MPI_COMM_WORLD)
```

### Локальные вычисления

```
1. Если local_data пуст -> возврат
2. Найти local_min, local_max
3. Получить global_min, global_max через MPI_Allreduce
4. Если global_max > global_min:
      range = global_max - global_min
      для каждого элемента:
          элемент = (элемент - global_min) × 255 / range
   Иначе:
      оставить без изменений
```

### Обработка особых случаев

- Пустой входной вектор: возвращается пустой результат
- Однородный вектор (min == max): растяжка не применяется, возвращается исходный
- n < P: некоторые процессы получают пустые блоки, корректно обрабатываются


## 5. Детали реализации

### Структура кода

**Используемые файлы:**
- `common/include/common.hpp` - общие типы данных
- `seq/include/ops_seq.hpp`, `seq/src/ops_seq.cpp` - последовательная версия
- `mpi/include/ops_mpi.hpp`, `mpi/src/ops_mpi.cpp` - параллельная версия
- `tests/functional/main.cpp` - функциональные тесты
- `tests/performance/main.cpp` - тесты производительности

**Классы:**
- `ShkenevIlinerStretchingHistIncreaseContrSEQ ` - последовательная реализация
- `ShkenevIlinerStretchingHistIncreaseContrMPI ` - параллельная реализация

**Методы:**
- `ValidationImpl()` - проверка входных данных
- `PreProcessingImpl()` - предобработка
- `RunImpl()` - основной алгоритм
- `PostProcessingImpl()` - проверка результата

**Вспомогательные функции MPI:**
- `CalculateDistribution` - вычисление распределения данных
- `FindLocalMinMax` - поиск локальных min/max
- `ApplyLinearStretching` - применение линейной растяжки

## 6. Экспериментальная установка

### Аппаратное обеспечение

- **Процессор:** Intel(R) Core(TM) 5 220H (2.70 GHz) 
- **ОЗУ:** 32,0 ГБ
- **ОС:** Windows 11 Домашняя для одного языка

### Программное обеспечение

- **Компилятор:** MSVC 14.44
- **MPI:** MS-MPI 10.0
- **Сборка:** Release 
- **CMake:** 4.2.0-rc1
- **Фреймворк тестирования:** Google Test

### Параметры тестирования

**Функциональные тесты:**
Было выполнено 11 тестов:
- [100, 150, 200] - базовый случай
- (1000 элементов) - большой вектор
- [0, 255] - минимальный контраст
- [0, 0, 255, 255] - дублирование значений
- (все 128) - однородный вектор
- (один элемент) - скалярный случай
- [127, 128] - близкий диапозон 
- [10, 20, 30] - три элемента
- [1, 2, 254] - асимметричный диапазон
- (5000 элементов) - огромный вектор
- пустой вектор

**Тесты производительности:**
- Размер вектора: 10 миллионов
- Диапазон значений: 50-200 (с добавлением 0 и 255)
- Число процессов: 1, 2, 4
- Режимы: task_run, pipeline

## 7. Результаты и обсуждение

### 7.1 Корректность

Корректность реализации проверена следующими способами:

1. **Функциональные тесты:** все 11 тестов успешно пройдены.
2. **Сравнение SEQ и MPI:** результаты идентичны на всех тестовых данных.
3. **Проверка размеров:** корректная обработка граничных случаев и результаты остаются в диапазоне [0, 255].

### 7.2 Производительность

Результаты измерений на векторе из 10 миллионов элементов

**task_run:**

| Режим | Процессов | Время, сек | Ускорение | Эффективность |
| ----- | --------- | ---------- | --------- | ------------- |
| seq   | 1         | 0.032      | 1.00      | N/A           |
| mpi   | 2         | 0.028      | 1.14      | 57%           |
| mpi   | 3         | 0.02       | 1.60      | 53%           |
| mpi   | 4         | 0.017      | 1.88      | 47%           |


**task_pipeline:**

| Режим | Процессов | Время, сек | Ускорение | Эффективность |
| ----- | --------- | ---------- | --------- | ------------- |
| seq   | 1         | 0.034      | 1.00      | N/A           |
| mpi   | 2         | 0.032      | 1.06      | 53%           |
| mpi   | 3         | 0.022      | 1.55      | 52%           |
| mpi   | 4         | 0.0177     | 1.92      | 48%           |



**Расчет показателей:**
- Ускорение = T_seq / T_mpi
- Эффективность = Ускорение / P x 100%

**Анализ результатов:**

1. **Сравнение SEQ и MPI:** 
- MPI-реализация демонстрирует ускорение уже на 2 процессах в обоих режимах
- Наибольшее ускорение достигается на 4 процессах

2. **Оптимальная конфигурация:** 
- Наибольшее ускорение наблюдается на 4 процессах
- Режим task_pipeline показывает чуть лучшее ускорение на 4 процессах (1.92 vs 1.88)

3. **Особенности алгоритма линейной растяжки:** 
- Хорошее масштабирование при переходе от 2 к 4 процессам
- Алгоритм хорошо подходит для параллелизации благодаря независимости обработки пикселей

**Вывод:**

- Линейная растяжка гистограммы хорошо параллелизуется с использованием вертикального разбиения данных
- Оптимальное число процессов — 4
- Эффективность снижается с ростом числа процессов из-за накладных расходов 
- Алгоритм демонстрирует хорошую масштабируемость для задач обработки больших изображений

## 8. Выводы

1. **Реализация:** 

Созданы и протестированы последовательная (SEQ) и параллельная (MPI) версии линейной растяжки гистограммы

2. **Корректность:** 

Все тесты пройдены, SEQ и MPI версии дают одинаковый результат

3. **Производительность:** 

MPI реализация показывает ускорение 1.9× на 4 процессах, алгоритм эффективно масштабируется до 4 процессов.

4. **Масштабируемость:** 

- Эффективность снижается при увеличении числа процессов из-за накладных расходов на коммуникацию
- Оптимальная конфигурация — 4 процесса для данного объема данных	


## 9. Список источников информации

1. Сысоев А. В. Лекции по параллельному программированию. — Н. Новгород: ННГУ, 2025.
2. Parallel Programming Course Slides - https://learning-process.github.io/parallel_programming_slides/
3. Microsoft. Microsoft MPI Documentation - https://learn.microsoft.com/en-us/message-passing-interface/microsoft-mpi 

## Приложение

### Фрагменты кода

**Последовательная версия (SEQ):**

```cpp
bool ShkenevIlinerStretchingHistIncreaseContrSEQ::RunImpl() {
  const auto &input = GetInput();
  auto &output = GetOutput();

  if (input.empty()) {
    output.clear();
    return true;
  }

  auto min_it = std::ranges::min_element(input);
  auto max_it = std::ranges::max_element(input);
  int min_val = *min_it;
  int max_val = *max_it;

  if (max_val > min_val) {
    int range = max_val - min_val;
    for (size_t i = 0; i < input.size(); ++i) {
      output[i] = (input[i] - min_val) * 255 / range;
    }
  } else {
    output = input;
  }

  return true;
}
```

**Параллельная версия (MPI):**
```cpp
bool ShkenevIlinerStretchingHistIncreaseContrMPI::RunImpl() {
  int rank = 0;
  int size = 0;
  MPI_Comm_rank(MPI_COMM_WORLD, &rank);
  MPI_Comm_size(MPI_COMM_WORLD, &size);

  int total_size = 0;
  if (rank == 0) {
    total_size = static_cast<int>(GetInput().size());
  }
  MPI_Bcast(&total_size, 1, MPI_INT, 0, MPI_COMM_WORLD);

  if (total_size == 0) {
    if (rank == 0) {
      GetOutput().clear();
    }
    return true;
  }

  auto [send_counts, displs] = CalculateDistribution(total_size, size);

  std::vector<int> local_data(send_counts[rank]);
  if (rank == 0) {
    MPI_Scatterv(GetInput().data(), send_counts.data(), displs.data(), MPI_INT, local_data.data(), send_counts[rank],
                 MPI_INT, 0, MPI_COMM_WORLD);
  } else {
    MPI_Scatterv(nullptr, send_counts.data(), displs.data(), MPI_INT, local_data.data(), send_counts[rank], MPI_INT, 0,
                 MPI_COMM_WORLD);
  }

  auto [local_min, local_max] = FindLocalMinMax(local_data);

  int global_min = INT_MAX;
  int global_max = INT_MIN;
  MPI_Allreduce(&local_min, &global_min, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);
  MPI_Allreduce(&local_max, &global_max, 1, MPI_INT, MPI_MAX, MPI_COMM_WORLD);

  ApplyLinearStretching(local_data, global_min, global_max);

  if (rank == 0) {
    GetOutput().resize(total_size);
  }

  MPI_Gatherv(local_data.data(), send_counts[rank], MPI_INT, (rank == 0) ? GetOutput().data() : nullptr,
              send_counts.data(), displs.data(), MPI_INT, 0, MPI_COMM_WORLD);

  return true;
}
```