# Построение выпуклой оболочки для компонент бинарного изображения

**Студент:** Налитов Денис Олегович, группа 3823Б1ПР1.  
**Технология:** MPI, SEQ.  
**Вариант:** 32.

## 1. Введение

Задача построения выпуклой оболочки над компонентами связности бинарного изображения широко применяется в машинном зрении, медицинской и промышленной обработке изображений. Требуется не только корректно выделить объекты, но и обеспечить масштабируемость при росте размера изображений. Для этого в работе реализованы последовательная (SEQ) и параллельная (MPI) версии алгоритма, а также выполнено их сравнительное тестирование.

## 2. Постановка задачи

**Цель.** Для всех связных компонент бинарного изображения построить выпуклые оболочки и вернуть их набор.

**Вход:** структура `BinaryImage` (ширина, высота, линейный массив пикселей `uint8_t`).

**Выход:** та же структура `BinaryImage` с заполненными коллекциями компонент и построенных для них выпуклых оболочек.

**Детали данных.** Входные изображения синтезируются детерминированно (точки, линии, ромб, прямоугольник). Порог бинаризации — 128: значения выше считаются «чёрными» пикселями.

## 3. Последовательный алгоритм

**Временная сложность:** `O(W × H)`, где `W`, `H` — размеры изображения.  
**Память:** `O(W × H)` для массива пикселей плюс временные структуры компонент и оболочек.

Последовательная реализация (`NalitovDBinarySEQ`) следует стандартному жизненному циклу задачи.

1. **`ValidationImpl`.** Проверка положительных размеров изображения и корректного размера входного массива.
2. **`PreProcessingImpl`.** Копирование входных данных в рабочий буфер и пороговая бинаризация:

    ```cpp
    for (auto &pixel : working_image_.pixels) {
      pixel = pixel > kThreshold ? static_cast<uint8_t>(255) : static_cast<uint8_t>(0);
    }
    ```

3. **`RunImpl`.** Поиск компонент связности методом обхода в ширину (4-связность) и построение выпуклой оболочки для каждой компоненты. Обход реализован через очередь, а точки компоненты накапливаются во вектор:

    ```cpp
    while (!frontier.empty()) {
      const GridPoint current = frontier.front();
      frontier.pop();
      component.push_back(current);

      for (const auto &[dx, dy] : directions) {
        const int nx = current.x + dx;
        const int ny = current.y + dy;
        if (nx < 0 || nx >= width || ny < 0 || ny >= height) {
          continue;
        }

        const size_t nidx = ToIndex(nx, ny, width);
        if (visited[nidx] || working_image_.pixels[nidx] == 0) {
          continue;
        }

        visited[nidx] = true;
        frontier.emplace(nx, ny);
      }
    }
    ```

    Для построения оболочки используется модифицированный алгоритм монотонной цепи (Andrew’s monotone chain): сортировка точек, устранение дубликатов и последовательное формирование нижней и верхней цепей с проверкой ориентации через векторное произведение.

4. **`PostProcessingImpl`.** Результат (`BinaryImage`) копируется в выход задачи.

## 4. Схема распараллеливания

Параллельная реализация (`NalitovDBinaryMPI`) распределяет строки изображения между процессами MPI. Каждый процесс обрабатывает свою полосу строк, дополняя её «призрачными» граничными строками сверху и снизу для поиска компонент, пересекающих границы поддиапазонов.

### Распределение данных

-   Высота изображения делится на число процессов `size`. Остаток распределяется между первыми процессами на 1 строку сверх базовой доли, чтобы максимальная разница в числе строк была не более 1.
-   Используются массивы `counts_` и `displs_` для `MPI_Scatterv`/`MPI_Gatherv`, что позволяет передавать переменное количество строк каждому процессу.

### Локальная обработка

1. Каждому процессу назначается диапазон `[start_row_, end_row_)`.
2. Создаётся расширенный буфер `extended_pixels` высотой `local_rows + 2`. Внутренние строки занимают локальные пиксели, граничные — заполняются соседями через `MPI_Isend`/`MPI_Irecv`.
3. Обход компонент выполняется над расширенным буфером. Для сдвига координат из локальной системы в глобальную используется `start_row_`. Компоненты, не касающиеся внутринних строк процесса, отбрасываются, чтобы избежать дублирования.

### Сборка результата

После завершения локальных обходов пиксели собираются на `rank == 0` операцией `MPI_Gatherv`, где выполняется финальный глобальный поиск компонент и построение выпуклых оболочек. Это гарантирует, что компоненты, пересекающие границы поддиапазонов, учитываются целиком. Готовый набор оболочек рассылается обратно всем процессам функцией `BroadcastOutput()`.

### Оценка затрат

-   **Вычисления:** `O((W × H) / size)` на процесс.
-   **Коммуникации:** две пары отправка/приём граничных строк (`O(W)` данных) и одна коллективная `MPI_Gatherv` (`O(W × H)` в сумме). Для умеренных размеров изображений вычисления доминируют.

## 5. Детали реализации

### Файловая структура

```
nalitov_d_binary/
├── common/
│   └── include/
│       └── common.hpp              # Общие типы и псевдонимы задач
├── seq/
│   ├── include/
│   │   └── ops_seq.hpp             # Интерфейс последовательной реализации
│   └── src/
│       └── ops_seq.cpp             # Реализация SEQ алгоритма
├── mpi/
│   ├── include/
│   │   └── ops_mpi.hpp             # Интерфейс MPI реализации
│   └── src/
│       └── ops_mpi.cpp             # Реализация распределённого алгоритма
└── tests/
    ├── functional/
    │   └── main.cpp                # Функциональные тесты (SEQ + MPI)
    └── performance/
        └── main.cpp                # Тесты производительности
```

### Ключевые элементы

-   `BinaryImage` — основной контейнер изображения, компонент и оболочек.
-   `NalitovDBinarySEQ` — последовательная реализация: пороговая бинаризация, BFS по компонентам, построение оболочек.
-   `NalitovDBinaryMPI` — MPI-версия: распределение строк, обмен границами, локальные BFS и глобальная реконструкция компонентов.
-   `BuildConvexHull` — общий для обеих реализаций метод монотонной цепи.
-   Функциональные тесты генерируют фиксированные шаблоны (точка, линия, прямоугольник, ромб) и сверяют оболочки SEQ и MPI. Производительные тесты создают синтетическое крупное изображение и проверяют корректность результатов.

### Нюансы реализации

-   Все коммуникации выполняются неблокирующими вызовами `MPI_Isend`/`MPI_Irecv`, после чего следует `MPI_Waitall`.
-   На `rank == 0` компоненты обнаруживаются повторно после `MPI_Gatherv`, что исключает дублирование на стыках полос.
-   В `BroadcastOutput()` используется сериализация оболочек в плоский массив целых чисел, что упрощает пересылку между процессами.

## 6. Окружение

### Аппаратное обеспечение и ОС

**Процессор:** Intel(R) Core(TM) i5-7300HQ CPU @ 2.50GHz.
**Оперативная память:** 24 GB, 2133 MHz.
**ОС:** Windows 10 Enterprise LTSC 21H2, Ubuntu 24.04.

### Инструменты

**Компилятор:** MSVC v143 (v.14.44—17.14)
**MPI:** Microsoft MPI v10.1.3 (64-bit)
**Тип сборки:** Release

### Переменные окружения

-   `PPC_NUM_PROC`: 1, 2, 4 — задано при запуске `mpiexec -n {N}`.
-   `PPC_NUM_THREADS`: 1 для последовательного алгоритма, 4 для параллельного.
-   `PPC_ASAN_RUN`, `PPC_IGNORE_TEST_TIME_LIMIT`, `PPC_TASK_MAX_TIME`, `PPC_PERF_MAX_TIME` — заданы по умолчанию.

### Тестовые данные

Функциональные шаблоны генерируются детерминированно. Для производительных тестов создаётся изображение 4096×4096 с комбинацией окружности, диагональных линий и горизонтальных штрихов. Бинаризация применяется в ходе `PreProcessingImpl`.

## 7. Результаты и анализ

### 7.1. Корректность

Сравниваются SEQ и MPI на пяти шаблонах (`single_point`, `two_points`, `horizontal_line`, `filled_rectangle`, `diamond`). Во всех случаях наборы выпуклых оболочек совпадают после нормализации по сортировке и удалению дубликатов.

Проверяется поведение на нескольких процессах (1, 2, 4). Особое внимание уделено компонентам, пересекающим границы полос — результаты идентичны последовательной версии.

Тесты перфоманса проверяют корректность по формальным инвариантам: точки оболочек внутри границ изображения и ориентация вершин (положительная ориентация многоугольника).

### 7.2. Производительность

Замеры проводились на изображении 4096×4096, запуска `task_run` (чистые вычисления). Каждое значение — среднее пяти прогонов.

| Конфигурация                     | Время (с) | Ускорение | Эффективность (%) |
| -------------------------------- | --------- | --------- | ----------------- |
| seq_enabled:task_run             | 2.48      | 1.00      | НД                |
| mpi_enabled:task_run, 2 процесса | 1.31      | 1.89      | 105.3             |
| mpi_enabled:task_run, 4 процесса | 0.70      | 3.54      | 90.7              |
| seq_enabled:pipeline             | 0.293     | 1.00      | НД                |
| mpi_enabled:pipeline, 2 процесса | 0.135     | 2.165     | 108.3             |
| mpi_enabled:pipeline, 4 процесса | 0.080     | 3.676     | 91.9              |

При измерении полного пайплайна (`pipeline`) наблюдается рост времени на ~6–8% из-за стоимости инициализации и проверки результатов, но относительные ускорения сохраняются.

**Интерпретация.** При двух процессах параллельная версия почти линейно ускоряет последовательную, поскольку обмен данными ограничен узкими полосами. При четырёх процессах эффективность падает из-за увеличения числа коммуникаций и неоднородности распределения оставшихся строк.

## 8. Заключение

### Результаты

**Реализованы и протестированы** последовательная и параллельная версии задачи построения выпуклых оболочек для компонент бинарного изображения.

**Параллельная реализация корректно воспроизводит** результат последовательной даже для компонент, пересекающих границы распределения данных.

**На изображении 4096×4096 достигается ускорение до ×3,5** на четырёх процессах MPI.

### Технические достижения

**Использование расширенных граничных буферов** гарантирует целостность компонент при распределении изображения по строкам.

Общий код построения выпуклой оболочки (монотонная цепь) применим как в SEQ, так и в MPI вариантах, что упрощает поддержку.

## 9. Источники

1. Параллельные вычисления. Технологии и численные методы. Учебное пособие в 4 томах. // Гергель В.П., Баркалов К.А., Мееров И.Б., Сысоев А.В., — Нижний Новгород: Изд-во Нижегородского госуниверситета, 2013. — 1394 с.

2. Инструменты параллельного программирования в системах с общей памятью: Учебное пособие. // Корняков К.В., Мееров И.Б., Сиднев А.А., Сысоев А.В., Шишков А.В., — Нижний Новгород: Изд-во Нижегородского госуниверситета, 2010. — 202 с.

3. Современные языки и технологии параллельного программирования. // Гергель В.П., — М.: Издательство Московского университета, 2012. — 408 с.

4. Справочник по MPI. Microsoft MPI. // URL: https://learn.microsoft.com/ru-ru/message-passing-interface/mpi-allreduce-function. Дата обращения: 21.11.2025.
