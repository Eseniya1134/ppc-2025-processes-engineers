# Ленточная горизонтальная схема - умножение матрицы на вектор

- Студент: Романов Михаил Павлович, группа 3823Б1ПР4
- Технология: SEQ | MPI
- Вариант: 11

## 1. Введение

Задача умножения матрицы на вектор является фундаментальной операцией в линейной алгебре, которая широко применяется в машинном обучении, численном моделировании и решении систем линейных уравнений.

При работе с матрицами больших объемов ($N \times M$), где $N$ и $M$ измеряются тысячами, последовательный подход становится неэффективным.Именно поэтому появляется необходимость в разработке параллельного алгоритма.

## 2. Постановка задачи

Дана матрица $A$ размера $N \times M$ и вектор $X$ размера $M$. Требуется вычислить вектор результата $B$ размера $N$, где каждый элемент $B_i$ является скалярным произведением $i$-й строки матрицы $A$ на вектор $X$
**Входные данные:** Матрица $A$ ($N \times M$), число строк $N$, число столбцов $M$ и вектор $X$ ($M$).
**Выходные данные** Вектор результата $B$ ($N$).

## 3. Базовый алгоритм (Последовательный)

Последовательный алгоритм выполняет прямое вычисление скалярного произведения для каждой строки матрицы $A$.

Инициализация. Вектор результата $B$ размера $N$ инициализируется.

Итерация. Внешний цикл итерируется по строкам, внутренний цикл вычисляет скалярное произведение строки $i$ на вектор $X$.
```cpp

 for (int i = 0; i < rows; ++i) {
  double temp = 0.0;
  for (int j = 0; j < cols; ++j) {
    const std::size_t idx =
        (static_cast<std::size_t>(i) * static_cast<std::size_t>(cols)) + static_cast<std::size_t>(j);
    temp += matrix[idx] * vec[static_cast<std::size_t>(j)];
  }
  res[static_cast<std::size_t>(i)] = temp;
}

```

## 4. Параллелизация

Для параллелизации используется ленточная горизонтальная схема, при которой строки матрицы $A$ распределяются по блокам между $P$ процессами.

Входная матрица $A$ делится на $P$ блоков строк. Распределение осуществляется корневым процессом (Rank 0) с помощью MPI_Scatterv, что позволяет обрабатывать блоки переменной длины в зависимости от остатка ($N \bmod P$) для обеспечения равномерной нагрузки.

Вычисление каждого элемента $B_i$ является независимым и требует доступа только к соответствующей строке $A_i$ и полному вектору $X$. Обмен граничными данными не требуется.

**1. Распределение данных:** Процесс 0 рассылает число строк $N$ и столбцов $M$ (MPI_Bcast).

Процесс 0 рассылает полный вектор $X$ всем процессам (MPI_Bcast).

Матрица $A$ распределяется по строкам с помощью MPI_Scatterv.

**2. Локальное вычисление:** Каждый процесс вычисляет скалярное произведение полученных строк $A_p$ на вектор $X$, получая локальный фрагмент результата $B_p$.


## 5. Детали реализации

**Функция распределения:** Вспомогательная функция CalculateDistribution вычисляет массивы counts и displs для обеспечения равномерного распределения строк матрицы $A$ и соответствующих элементов вектора $B$ между процессами.

**Коммуникация:** Для распределения матрицы $A$ используется MPI_Scatterv, для сбора результатов — MPI_Allgatherv, а для вектора $X$ — MPI_Bcast.

**Детали тестирования:** Набор функциональных тестов (kTestParam) включает в себя 8 сценариев, охватывающих различные краевые и типовые случаи, включая квадратные, прямоугольные, и минимальные размеры: $(6, 6), (4, 2), (2, 4), (1, 1), (2, 1), (1, 2), (2, 2), (3, 7)$. 

Генерация данных:

Вектор $X$ заполняется константным значением $\mathbf{1.0}$.

Матрица $A$ заполняется по формуле $A_{ij} = i + j$.

Ожидаемый результат: Вектор $B$ вычисляется как сумма элементов соответствующей строки $A$, поскольку $X_j = 1$.

Проверка корректности: Используется абсолютный допуск $\mathbf{1e-5}$ для сравнения результатов с плавающей точкой между последовательной и параллельной версиями.

**Тесты производительности:**
Тесты производительности проводятся на фиксированном размере матрицы $\mathbf{N=8000}$ и $\mathbf{M=8000}$(Для более показательного результата были взяты большие значения чем в нынешних тестах). Вектор $X$ заполняется константным значением 0.5, а матрица $A$ заполняется случайными значениями.

## 6. Тестовое окружение
* Аппаратное обеспечение/Операционная система: AMD Ryzen 5 7640HS, 6 ядер/12 потоков, LPDDR5X 16 GB, Windows 11.
* Инструменты сборки: Visual studio 2022 community release. CMake 4.2.0.
* Переменные окружения: PPC_NUM_THREADS=PPC_NUM_PROC=1/2/4/6/8/12/16, PPC_PERF_MAX_TIME=10000.

## 7. Результаты

### 7.1 Корректность
Корректность проверялась с использованием набора функциональных тестов, включающих малые матрицы и большие матрицы для проверки краевых условий. Проверена точность вычислений с плавающей точкой (допуск $1e-5$).

Результаты параллельной версии (MPI) полностью совпали с результатами последовательной (SEQ) реализации для всех тестовых случаев.
### 7.2 Производительность
|Режим|Число процессов|Время(мс)|Ускорение|Эффективность|
|-----|---------------|---------|---------|-|
|seq|1|645|1||
|mpi|4|884|0.73|18.3%|
|mpi|8|1158|0.56|7%|
|mpi|12|1566|0.41|3.4%|
|mpi|16|3207|0.20|1.3%|

Наблюдается анти-ускорение (деградация производительности): время выполнения монотонно растет с увеличением числа процессов, и, начиная с $P=4$, ускорение $S_p$ становится меньше 1. Это означает, что параллельная версия работает медленнее, чем последовательная. В теории это связано с слишком большими расходами на mpi реализацию и отсутствием у процессора стольки ядер, что и влечёт такую производительность.

### 8. Заключение
Была разработана параллельная реализация задачи умножения матрицы на вектор с использованием MPI и горизонтальной ленточной схемы.
Вопреки ожиданиям для задачи с высокой интенсивностью вычислений, тестирование показало сильную деградацию производительности (анти-ускорение). Основные причины такого поведения:

**Высокие накладные расходы MPI:** Коммуникационные издержки (Bcast, Scatterv, Allgatherv) для данной размерности задачи оказались больше, чем выигрыш от параллелизации.

**Конкуренция за ресурсы:** При $P > 12$ (превышение числа логических потоков процессора) резко возросли накладные расходы на контекстное переключение и конкуренцию за общую память на одноузловой машине, что привело к значительному увеличению времени выполнения.

### 9. Источники

1.  Microsoft MPI : документация [Электронный ресурс] // Microsoft Learn. – URL: https://learn.microsoft.com/ru-ru/message-passing-interface/microsoft-mpi
2. Сысоев А. В. Курс лекций по параллельному программированию.
3. Parallel programming course [Электронный ресурс] : документация URL: https://learning-process.github.io/parallel_programming_course/ru/common_information/processes_tasks.html