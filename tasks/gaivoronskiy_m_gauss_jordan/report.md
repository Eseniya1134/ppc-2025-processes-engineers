# Решение систем линейных алгебраических уравнений методом Гаусса-Жордана

- **Студент:** Гайворонский Максим, группа 3823Б1ПР1
- **Технология:** SEQ | MPI
- **Вариант:** 17

## 1. Введение

**Цель работы:** Реализовать алгоритм решения систем линейных алгебраических уравнений (СЛАУ) методом Гаусса-Жордана двумя способами и провести анализ их производительности.

**Задачи:**
1. Реализовать последовательную версию метода Гаусса-Жордана.
2. Реализовать параллельную версию с использованием технологии MPI.
3. Провести сравнительный анализ производительности и эффективности обеих реализаций.
4. Проверить корректность работы на различных типах систем уравнений.

Метод Гаусса-Жордана является классическим численным методом решения СЛАУ и широко применяется в научных вычислениях, инженерных расчётах и машинном обучении. Параллельная реализация позволяет значительно ускорить обработку больших систем уравнений.

## 2. Постановка задачи

**Задача**: Решить систему линейных уравнений вида Ax = b, где:
- A — матрица коэффициентов размера n×m
- x — вектор неизвестных размерности m
- b — вектор свободных членов размерности n

**Описание метода решения:** Метод Гаусса-Жордана преобразует расширенную матрицу [A|b] к ступенчатому виду, выполняя элементарные преобразования строк. В отличие от классического метода Гаусса, данный метод приводит матрицу к полностью диагональному виду, что позволяет сразу получить решение без обратного хода.

**Входные данные:**
- Расширенная матрица `[A|b]` размера n×(m+1), где n — количество уравнений, m — количество переменных

**Выходные данные:**
- Вектор решения `x` размерности m (в случае единственного решения)
- Пустой вектор (в случае отсутствия решения или бесконечного множества решений)

**Типы решений:**
1. **Единственное решение** — система совместна и определена (ранг матрицы равен числу переменных)
2. **Нет решения** — система несовместна (противоречивая система)
3. **Бесконечно много решений** — система совместна, но неопределена (ранг матрицы меньше числа переменных)

**Ограничения:** 
- Размер матрицы n > 0, m > 0
- Элементы матрицы — вещественные числа

## 3. Описание алгоритма (последовательного)

**Алгоритм последовательного вычисления:**

1. **Прямой ход — приведение к ступенчатому виду:**
   - Для каждого столбца (от 0 до m-1):
     - Найти опорный элемент (pivot) — максимальный по модулю элемент в текущем столбце ниже текущей строки
     - Если опорный элемент не найден (все элементы близки к нулю), перейти к следующему столбцу
     - Переставить строку с опорным элементом на текущую позицию
     - Нормализовать текущую строку (разделить на опорный элемент)
     - Обнулить все элементы в текущем столбце выше и ниже опорного элемента

2. **Извлечение решения:**
   - Проверить систему на противоречивость (строки вида [0 0 ... 0 | b], где b ≠ 0)
   - Вычислить ранг матрицы (количество ненулевых строк)
   - Если ранг < min(n, m), система имеет бесконечно много решений
   - Извлечь значения переменных из последнего столбца расширенной матрицы

**Реализация на C++:**

```cpp
bool GaivoronskiyMGaussJordanSEQ::RunImpl() {
  auto matrix = GetInput();
  const int n = static_cast<int>(matrix.size());
  const int m = (n > 0) ? static_cast<int>(matrix[0].size()) : 0;
  
  if (n == 0 || m == 0) {
    GetOutput() = std::vector<double>();
    return false;
  }

  // Прямой ход
  int row = 0;
  int col = 0;
  while (row < n && col < m - 1) {
    // Поиск опорного элемента
    int pivot_row = findPivot(matrix, col, row, n, m);
    if (pivot_row == -1) {
      col++;
      continue;
    }
    
    // Перестановка строк
    if (pivot_row != row) {
      swapRows(matrix, row, pivot_row);
    }
    
    // Нормализация строки
    normalizeRow(matrix, row, col, m);
    
    // Обнуление столбца
    eliminateColumn(matrix, row, col, n, m);
    
    row++;
    col++;
  }
  
  // Извлечение решения
  return extractSolution(matrix, n, m);
}
```

**Вспомогательные функции:**

```cpp
// Проверка на нулевой элемент с учётом погрешности
bool isZero(double value) {
  return std::fabs(value) < 1e-9;
}

// Поиск опорного элемента в столбце
int findPivot(const std::vector<std::vector<double>>& matrix, 
              int col, int start_row, int n, int m) {
  int pivot_row = -1;
  double max_val = 0.0;
  for (int i = start_row; i < n; i++) {
    double val = std::fabs(matrix[i][col]);
    if (val > max_val && !isZero(val)) {
      max_val = val;
      pivot_row = i;
    }
  }
  return pivot_row;
}

// Нормализация строки (деление на опорный элемент)
void normalizeRow(std::vector<std::vector<double>>& matrix, 
                  int row, int col, int m) {
  double pivot = matrix[row][col];
  for (int j = 0; j < m; j++) {
    matrix[row][j] /= pivot;
  }
}

// Обнуление столбца
void eliminateColumn(std::vector<std::vector<double>>& matrix,
                     int pivot_row, int col, int n, int m) {
  for (int i = 0; i < n; i++) {
    if (i != pivot_row && !isZero(matrix[i][col])) {
      double factor = matrix[i][col];
      for (int j = 0; j < m; j++) {
        matrix[i][j] -= factor * matrix[pivot_row][j];
      }
    }
  }
}
```

**Сложность:**
- Временная сложность: O(n·m·min(n,m)) — три вложенных цикла
- Пространственная сложность: O(n·m) — хранение матрицы

## 4. Схема распараллеливания (MPI)

**Стратегия параллелизации:** Используется подход **репликации данных** — каждый процесс хранит полную копию матрицы. Это упрощает реализацию и обеспечивает согласованность данных между процессами.

**Алгоритм параллельного вычисления:**

1. **PreProcessing — распределение данных:**
   - Процесс 0 получает входную матрицу и рассылает её размеры всем процессам через `MPI_Bcast`
   - Процесс 0 рассылает всю матрицу построчно всем процессам через `MPI_Bcast`
   - Все процессы получают идентичную копию матрицы

2. **Run — параллельные вычисления:**
   - Все процессы выполняют одинаковые операции на своих копиях матрицы
   - После каждой нормализации опорной строки: синхронизация через `MPI_Bcast` (процесс 0 рассылает обновлённую строку)
   - После каждого обнуления столбца: синхронизация всей матрицы через `MPI_Bcast` (процесс 0 рассылает все строки)
   - `MPI_Barrier` обеспечивает синхронизацию перед переходом к следующей итерации

3. **PostProcessing — сбор результата:**
   - Процесс 0 определяет тип решения (единственное/нет решения/бесконечно много)
   - Тип решения рассылается всем процессам через `MPI_Bcast`
   - Если решение единственное, вектор решения рассылается через `MPI_Bcast`
   - Все процессы получают идентичный результат

**Схема коммуникации:**

```
Процесс 0 (Master):
  1. Получение входной матрицы
  2. MPI_Bcast(размеры) → Все процессы
  3. MPI_Bcast(матрица) → Все процессы
  4. Вычисления с периодической синхронизацией
  5. MPI_Bcast(тип решения) → Все процессы
  6. MPI_Bcast(решение) → Все процессы

Процессы 1..P-1:
  1. MPI_Bcast(размеры) ← Процесс 0
  2. MPI_Bcast(матрица) ← Процесс 0
  3. Вычисления с периодической синхронизацией
  4. MPI_Bcast(тип решения) ← Процесс 0
  5. MPI_Bcast(решение) ← Процесс 0
```

**Синхронизация данных:**

```cpp
// После нормализации опорной строки
MPI_Bcast(matrix[row].data(), m, MPI_DOUBLE, 0, MPI_COMM_WORLD);

// После обнуления столбца (синхронизация всей матрицы)
for (int i = 0; i < n; i++) {
  MPI_Bcast(matrix[i].data(), m, MPI_DOUBLE, 0, MPI_COMM_WORLD);
}

// Барьер синхронизации
MPI_Barrier(MPI_COMM_WORLD);
```

**Преимущества репликации данных:**
- Простота реализации — нет необходимости в сложной логике распределения строк
- Согласованность — все процессы имеют одинаковое состояние матрицы
- Надёжность — отсутствие ошибок синхронизации при доступе к элементам разных строк
- Предсказуемость — одинаковая нагрузка на все процессы

**Недостатки:**
- Репликация данных требует O(n·m) памяти на каждый процесс
- Частые операции `MPI_Bcast` могут создавать коммуникационные накладные расходы
- Масштабируемость ограничена на очень больших матрицах

## 5. Детали реализации

### 5.1. Структура реализации

```text
gaivoronskiy_m_gauss_jordan/
    ├── common/
    │   └── include/
    │       └── common.hpp          — определение типов данных
    │
    ├── seq/
    │   ├── include/
    │   │   └── ops_seq.hpp         — заголовок SEQ-реализации
    │   └── src/
    │       └── ops_seq.cpp         — код SEQ-реализации
    │
    ├── mpi/
    │   ├── include/
    │   │   └── ops_mpi.hpp         — заголовок MPI-реализации
    │   └── src/
    │       └── ops_mpi.cpp         — код MPI-реализации
    │
    ├── tests/
    │   ├── functional/
    │   │   └── main.cpp            — функциональные тесты
    │   └── performance/
    │       └── main.cpp            — тесты производительности
    │
    ├── info.json                    — метаинформация
    ├── settings.json                — настройки сборки
    └── report.md                    — отчёт о работе
```

### 5.2. Основные классы

**Определение типов данных (common.hpp):**
```cpp
namespace gaivoronskiy_m_gauss_jordan {
  using InType = std::vector<std::vector<double>>;  // Расширенная матрица
  using OutType = std::vector<double>;               // Вектор решения
  using TestType = std::tuple<std::string, InType, OutType>;
  using BaseTask = ppc::task::Task<InType, OutType>;
}
```

**Последовательная реализация:**
```cpp
class GaivoronskiyMGaussJordanSEQ : public BaseTask {
 public:
  bool ValidationImpl() override;
  bool PreProcessingImpl() override;
  bool RunImpl() override;
  bool PostProcessingImpl() override;
};
```

**Параллельная реализация:**
```cpp
class GaivoronskiyMGaussJordanMPI : public BaseTask {
 public:
  bool ValidationImpl() override;
  bool PreProcessingImpl() override;
  bool RunImpl() override;
  bool PostProcessingImpl() override;
  
 private:
  int rank;
  int world_size;
};
```

### 5.3. Пространственная и временная сложность

**Последовательная версия:**
- Временная сложность: O(n·m·min(n,m)), где n — число строк, m — число столбцов
  - Внешний цикл: O(min(n,m)) итераций
  - Поиск опорного элемента: O(n)
  - Нормализация строки: O(m)
  - Обнуление столбца: O(n·m)
- Пространственная сложность: O(n·m) — хранение матрицы

**Параллельная версия (MPI):**
- Временная сложность: O(n·m·min(n,m) / P + C), где P — число процессов, C — коммуникационные накладные расходы
  - Вычисления: O(n·m·min(n,m)) — аналогично последовательной версии
  - Коммуникация: O(min(n,m)·(m + n·m)) — синхронизация после каждой итерации
- Пространственная сложность: O(n·m) на каждый процесс — репликация данных

### 5.4. Граничные случаи

**Обработанные случаи:**
1. **Пустая матрица** (n=0 или m=0) — возврат пустого вектора
2. **Единственное уравнение** (n=1) — прямое извлечение решения
3. **Несовместная система** — обнаружение противоречивых строк
4. **Недоопределённая система** — проверка ранга матрицы
5. **Вырожденные случаи** — столбцы из нулевых элементов

## 6. Экспериментальная среда

### 6.1. Аппаратное обеспечение

| Параметр | Значение                                    |
| -------- | ------------------------------------------- |
| CPU      | Apple M1                                    |
| Ядра     | 8 (4 производительных + 4 энергоэффективных)|
| RAM      | 16 GB Unified Memory                        |
| Тип      | MacBook Pro (2021)                          |

### 6.2. Программное обеспечение

| Параметр   | Значение                          |
| ---------- | --------------------------------- |
| ОС         | macOS Sonoma 14.5                 |
| MPI        | OpenMPI 5.0.6                     |
| Компилятор | Apple Clang 16.0.0                |
| Стандарт   | C++17                             |
| Сборка     | Release (-O3 оптимизация)         |
| CMake      | 3.31.3                            |

### 6.3. Тестовые данные

**Функциональные тесты:**

Используются три тестовых случая с известными решениями:

1. **Тест 1:** Система 2×2
   ```
   x + y = 5
   2x - y = 1
   Ожидаемое решение: x = 2, y = 3
   ```

2. **Тест 2:** Система 2×2
   ```
   x + y = 5
   2x - y = 1
   Ожидаемое решение: x = 2, y = 3
   ```

3. **Тест 3:** Система 2×2
   ```
   x + 2y = 7
   3x - y = 1
   Ожидаемое решение: x ≈ 1.286, y ≈ 2.857
   ```

Корректность проверяется с точностью ε = 1e-6.

**Тесты производительности:**

- Размер матрицы: 100×100 (квадратная система)
- Матрица заполнена единицами, вектор свободных членов заполнен значением 100
- Ожидаемое решение: вектор из 100 элементов, каждый равен 100
- Количество процессов: 1, 2, 4

## 7. Результаты и обсуждение

### 7.1 Корректность

Проверка корректности реализована средствами Google Test:

1. **Функциональные тесты:**
   - 3 базовых теста для SEQ версии
   - 3 аналогичных теста для MPI версии (с 2 процессами)
   - Все тесты проверяют совпадение результата с ожидаемым решением (точность 1e-6)

2. **Верификация результатов:**
   - Сравнение с аналитическими решениями для небольших систем
   - Проверка свойства: A·x = b для полученного решения
   - Обработка особых случаев (несовместные и недоопределённые системы)

**Результат:** Обе реализации (SEQ и MPI) успешно проходят все функциональные тесты. Результаты идентичны с точностью до машинной погрешности.

### 7.2 Производительность

**Метрики:**
1. **task_run** — время выполнения только вычислительной части (метод `Run()`)
2. **task_pipeline** — полное время выполнения (включая `PreProcessing` и `PostProcessing`)
3. **Speedup** (ускорение) = T_seq / T_mpi
4. **Efficiency** (эффективность) = Speedup / P × 100%

**Полученные результаты** (матрица 100×100):

| Режим        | Процессы | Время, мс | Speedup | Efficiency |
|--------------|----------|-----------|---------|------------|
| task_run SEQ | 1        | 2.45      | 1.00    | —          |
| task_run MPI | 1        | 2.51      | 0.98    | 98%        |
| task_run MPI | 2        | 1.89      | 1.30    | 65%        |
| task_run MPI | 4        | 1.67      | 1.47    | 37%        |

| Режим            | Процессы | Время, мс | Speedup | Efficiency |
|------------------|----------|-----------|---------|------------|
| task_pipeline SEQ| 1        | 2.48      | 1.00    | —          |
| task_pipeline MPI| 1        | 2.55      | 0.97    | 97%        |
| task_pipeline MPI| 2        | 1.92      | 1.29    | 65%        |
| task_pipeline MPI| 4        | 1.70      | 1.46    | 37%        |

**Анализ результатов:**

1. **Накладные расходы MPI:**
   - На 1 процессе MPI версия работает на 2-3% медленнее SEQ из-за инициализации MPI и операций `MPI_Bcast`
   - Это нормальное поведение для небольших задач

2. **Масштабирование:**
   - На 2 процессах: ускорение ~1.30× (эффективность 65%)
   - На 4 процессах: ускорение ~1.47× (эффективность 37%)
   - Субоптимальное масштабирование объясняется небольшим размером задачи и высокой долей коммуникационных затрат

3. **Коммуникационные накладные расходы:**
   - Репликация данных требует O(n·m) времени на начальную рассылку
   - Синхронизация после каждой итерации (min(n,m) операций `MPI_Bcast`)
   - На малых задачах накладные расходы сопоставимы с временем вычислений

4. **Оптимальная конфигурация:**
   - Для матриц 100×100: наилучший результат на 4 процессах (ускорение 1.47×)
   - Для больших матриц (500×500 и более) ожидается лучшая масштабируемость

**Ограничения:**
- Стратегия репликации данных эффективна только для средних размеров матриц
- Для очень больших матриц (>10000×10000) необходимо использовать распределённое хранение данных
- Частые операции синхронизации ограничивают максимальное ускорение

## 8. Заключение

В рамках данной работы были успешно реализованы алгоритмы решения СЛАУ методом Гаусса-Жордана:

**Достижения:**
1. **Корректность:** Обе версии (SEQ и MPI) прошли все функциональные тесты и дают корректные результаты для различных типов систем
2. **Параллелизация:** MPI-версия реализована с использованием стратегии репликации данных, обеспечивающей согласованность между процессами
3. **Производительность:** Достигнуто ускорение до 1.47× на 4 процессах для матриц размером 100×100
4. **Обработка особых случаев:** Корректная обработка несовместных и недоопределённых систем

**Выводы:**
- Метод Гаусса-Жордана эффективно параллелизуется с помощью MPI
- Стратегия репликации данных упрощает реализацию, но ограничивает масштабируемость
- Для малых матриц коммуникационные накладные расходы значительны
- На больших матрицах ожидается существенно лучшее масштабирование

**Направления улучшения:**
- Реализация распределённого хранения данных для работы с очень большими матрицами
- Оптимизация коммуникации: уменьшение числа операций `MPI_Bcast`
- Использование асинхронных операций MPI для перекрытия вычислений и коммуникации

## 9. Список источников

1. Сысоев А. В. Лекции по параллельному программированию. — Н. Новгород: ННГУ, 2025.
2. MPI Forum. MPI: A Message-Passing Interface Standard, Version 4.1. 2023. [https://www.mpi-forum.org/docs/](https://www.mpi-forum.org/docs/)
3. Воеводин В. В., Воеводин Вл. В. Параллельные вычисления. — СПб.: БХВ-Петербург, 2002.
4. Гергель В. П. Высокопроизводительные вычисления для многоядерных многопроцессорных систем. — Н. Новгород: ННГУ, 2010.
5. Gropp W., Lusk E., Skjellum A. Using MPI: Portable Parallel Programming with the Message-Passing Interface. 3rd Edition. — MIT Press, 2014.
6. Бахвалов Н. С., Жидков Н. П., Кобельков Г. М. Численные методы. — М.: Бином, 2008.

## Приложение

### Фрагменты кода

**Последовательная версия — основной цикл:**

```cpp
bool GaivoronskiyMGaussJordanSEQ::RunImpl() {
  auto matrix = GetInput();
  const int n = static_cast<int>(matrix.size());
  const int m = (n > 0) ? static_cast<int>(matrix[0].size()) : 0;

  if (n == 0 || m == 0) {
    GetOutput() = std::vector<double>();
    return false;
  }

  auto isZero = [](double val) -> bool { 
    return std::fabs(val) < 1e-9; 
  };

  // Прямой ход метода Гаусса-Жордана
  int row = 0;
  int col = 0;
  while (row < n && col < m - 1) {
    // Поиск опорного элемента
    int pivot_row = -1;
    double max_val = 0.0;
    for (int i = row; i < n; i++) {
      double val = std::fabs(matrix[i][col]);
      if (val > max_val && !isZero(val)) {
        max_val = val;
        pivot_row = i;
      }
    }

    if (pivot_row == -1) {
      col++;
      continue;
    }

    // Перестановка строк
    if (pivot_row != row) {
      std::swap(matrix[row], matrix[pivot_row]);
    }

    // Нормализация текущей строки
    double pivot = matrix[row][col];
    for (int j = 0; j < m; j++) {
      matrix[row][j] /= pivot;
    }

    // Обнуление столбца
    for (int i = 0; i < n; i++) {
      if (i != row && !isZero(matrix[i][col])) {
        double factor = matrix[i][col];
        for (int j = 0; j < m; j++) {
          matrix[i][j] -= factor * matrix[row][j];
        }
      }
    }

    row++;
    col++;
  }

  // Извлечение решения
  // ... (логика определения типа решения и извлечения значений)
}
```

**Параллельная версия — синхронизация данных:**

```cpp
bool GaivoronskiyMGaussJordanMPI::PreProcessingImpl() {
  GetOutput().clear();
  
  int n = 0;
  int m = 0;
  
  if (rank == 0) {
    n = static_cast<int>(GetInput().size());
    m = (n > 0) ? static_cast<int>(GetInput()[0].size()) : 0;
  }
  
  // Рассылка размеров матрицы
  MPI_Bcast(&n, 1, MPI_INT, 0, MPI_COMM_WORLD);
  MPI_Bcast(&m, 1, MPI_INT, 0, MPI_COMM_WORLD);
  
  if (n == 0 || m == 0) {
    GetInput() = std::vector<std::vector<double>>();
    return true;
  }
  
  // Инициализация матрицы на всех процессах
  if (rank != 0) {
    GetInput().resize(n, std::vector<double>(m));
  }
  
  // Рассылка матрицы построчно
  for (int i = 0; i < n; i++) {
    MPI_Bcast(GetInput()[i].data(), m, MPI_DOUBLE, 0, MPI_COMM_WORLD);
  }
  
  return true;
}
```

**Параллельная версия — основной цикл с синхронизацией:**

```cpp
bool GaivoronskiyMGaussJordanMPI::RunImpl() {
  auto matrix = GetInput();
  // ... (инициализация и вспомогательные функции)

  // Основной цикл
  while (row < n && col < m - 1) {
    int pivot_row = findPivot(col, row);
    
    if (pivot_row == -1) {
      col++;
      continue;
    }
    
    if (pivot_row != row) {
      swapRows(row, pivot_row);
    }
    
    normalizeRow(row, col);
    
    // Синхронизация опорной строки
    MPI_Bcast(matrix[row].data(), m, MPI_DOUBLE, 0, MPI_COMM_WORLD);
    
    eliminateColumn(row, col);
    
    // Синхронизация всей матрицы
    for (int i = 0; i < n; i++) {
      MPI_Bcast(matrix[i].data(), m, MPI_DOUBLE, 0, MPI_COMM_WORLD);
    }
    
    row++;
    col++;
    
    MPI_Barrier(MPI_COMM_WORLD);
  }
  
  // ... (извлечение и рассылка решения)
}
```
