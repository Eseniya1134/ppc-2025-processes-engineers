# Сумма элементов матрицы

- Студент: Шакирова Есения Андреевна, группа 3823Б1ПР2
- Технология: SEQ | MPI
- Вариант: 10


## 1. Введение

Вычисление суммы всех элементов матрицы — базовая операция, которая используется во многих прикладных задачах: обработке данных, научных расчётах, анализе информации. Хотя задача и выглядит простой, при работе с большими объёмами данных важно правильно организовать вычисления и распределить нагрузку между процессорами.

В этой работе разработаны последовательный (SEQ) и параллельный (MPI) алгоритмы для подсчёта суммы элементов матрицы. Последовательная версия обрабатывает элементы на одном процессоре, параллельная распределяет данные между процессами с использованием MPI. Обе реализации интегрированы в фреймворк PPC и протестированы.


## 2. Постановка задачи

Дана прямоугольная матрица \(A\) размера \(n \times m\), содержащая целые числа. Для данной матрицы требуется вычислить сумму ее элементов.

**Входные данные:** Два целых числа \(n\) и \(m\), задающих соответственно кол-во строк и столбцов матрицы, \(n \times m\) чисел задающих элементы матрицы: size_t rows, size_t cols, std::vector<int64_t> data;

**Выходные данные:** Целое число: using OutType = int64_t - сумма всех элементов матрицы.


## 3. Описание алгоритма

### 3.1. Базовый алгоритм (Последовательная версия SEQ)

**Алгоритм работы SEQ-версии:**
1. Проверка корректности входных данных (размеры больше нуля, количество элементов соответствует размерности).
2. Инициализация суммы нулём.
3. Двойной цикл по всем элементам матрицы с накоплением суммы.
```cpp
for (size_t i = 0; i < GetInput().rows; i++) {
    for (size_t j = 0; j < GetInput().cols; j++) {
      GetOutput() += GetInput().at(i, j);
    }
} 
```
4. Сохранение результата в `Output`.

Внешний цикл проходит по строкам, внутренний — по столбцам. Каждый элемент последовательно добавляется к общей сумме.

**Сложность:**
- Временная: O(N × M), где N — количество строк, M — количество столбцов;
- Пространственная: O(1) дополнительной памяти.


### 3.2. Описание параллельного алгоритма (Версия MPI)

**Схема распараллеливания**
Параллельная версия алгоритма основана на декомпозиции матрицы по строкам. Каждый процесс обрабатывает свою часть строк и вычисляет частичную сумму, после чего результаты агрегируются с помощью операции редукции.

**Распределение нагрузки:**
```cpp
size_t rows_per_process = row_count / p_count;
size_t remaining_rows = row_count % p_count;

size_t my_rows = rows_per_process;
if (rank < static_cast<int>(remaining_rows)) {
  my_rows = my_rows + 1;
}
```
Если строки не делятся нацело, остаток распределяется между первыми процессами. Таким образом, разница в количестве обрабатываемых строк между процессами не превышает одной строки.

**Этапы работы параллельного алгоритма:**
- Рассылка размеров через `MPI_Bcast` — все процессы узнают размерность матрицы;
- Распределение данных через `MPI_Scatterv` — каждый процесс получает свою часть строк;
- Локальные вычисления — каждый процесс суммирует элементы в своей части;
- Агрегация результатов через `MPI_Allreduce` — частичные суммы объединяются в итоговую.

**Сложность:**
- Временная: O((N × M) / P + log P), где P — количество процессов;
  - O((N × M) / P) — время локальных вычислений;
  - O(log P) — время коллективных операций MPI.
- Пространственная: O((N × M) / P) на каждом процессе.


## 4. Детали реализации

Для функциональных тестов входные данные читаются из текстовых файлов, расположенных в каталоге `data/`. Файлы содержат размеры матрицы, её элементы и ожидаемое значение суммы, что позволяет автоматически проверять корректность работы.

Дополнительно механизм загрузки использует абсолютный путь, формируемый через `GetAbsoluteTaskPath`, что гарантирует корректное нахождение тестовых файлов вне зависимости от рабочей директории и обеспечивает воспроизводимость тестов в разных средах исполнения.


## 5. Тестовое окружение

- Аппаратное обеспечение/Операционная система: Intel(R) Core(TM) i5 5200U, 2P+4E ядер, 8Gb Ddr3 1600Mhz, Windows 10, MS-MPI.
- Инструменты сборки: Cmake 4.2.0-rc4, Visual Studio 2022, MSVC, x64 Release.
- Переменные окружения: Использовались настройки по умолчанию. Performance-тесты для MPI запускались с количеством процессов 1, 2 и 4 через `mpiexec`, переменная `PPC_PERF_MAX_TIME` не изменялась.
- Данные: вручную созданные тесты небольшого размера


## 6. Результаты

### 6.1 Корректность

Корректность работы реализации была подтверждена с помощью набора функциональных тестов, включающих матрицы различного размера и структуры. В тестировании использовались: матрицы с одной строкой и одним столбцом, матрицы, полностью состоящие из нулевых значений, а также прямоугольные (неквадратные) матрицы. Во всех случаях последовательная и параллельная версии давали одинаковый корректный результат.

### 6.2 Производительность

Время, ускорение, эффективность:

| Режим        | Кол-во процессов | Время, сек | Ускорение | Эффективность параллелизма |
|-------------|-------|---------|---------|------------|
| SEQ         | 1     | 0.106  | 1.00    | N/A        |
| MPI         | 2     | 0.230  | 0.48   | 24.7%      |
| MPI         | 4     | 0.175  | 0.61   | 15.2%      |
| MPI         | 6     | 0.162  | 0.65   | 10.9%      |
| MPI         | 8     | 0.154  | 0.70   | 8.7%       |

![Ускорение от количества процессов](./report_assets/pic1.png)
![Эффективность параллелизма](./report_assets/pic2.png)

Первое, что стоит отметить, это замедление MPI версии относительно последовательной. Данных для обработки много, в то время как операций над ними мало. Большая часть времени тратится на пересылку данных между процессами.

При отсутствии рассылки входных данных удавалось достичь ускорения относительно последовательной версии в 5.2 раза, что не являлось пределом, так как то решение не было оптимизировано и хранило полную копию входных данных на каждом из процессов.

Видно, что производительность растет до достижения 12 процессов, что равно количеству производительных ядер процессора с учетом Hyper-Threading. Ускорение в максимуме достигло 2.54 раз относительно запуска с одним MPI процессом.

Далее до 20 процессов наблюдается плато. После 20 процессов размер работы, проделанной каждым процессом продолжает уменьшаться, но накладные расходы на пересылки растут медленнее, что позволяет выжать еще немного ускорения.