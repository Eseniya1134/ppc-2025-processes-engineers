# Итеративные методы (Зейделя)

-   **Студент** Карпич Иван Валерьевич
-   **Группа** 3823Б1ПР4
-   **Технология** SEQ|MPI
-   **Вариант** 19

## 1. Введение

Решение систем линейных уравнений является фундаментальной задачей в численных методах и вычислительной математике. Метод Зейделя является одним из итерационных методов, позволяющих найти решение системы линейных уравнений Ax = b с заданной точностью.

Преимущество метода Зейделя перед прямыми методами (например, методом исключения Гаусса) состоит в его простоте реализации и возможности параллелизации. Метод Зейделя часто используется для больших разреженных матриц, где прямые методы могут быть неэффективны.

Цель данной работы: реализация последовательного и параллельного алгоритмов решения системы линейных уравнений методом Зейделя с использованием технологии MPI, а также анализ производительности параллельной реализации.

## 2. Постановка задачи

Требуется решить систему линейных уравнений вида:

Ax = b

где:

-   A - квадратная матрица размером n × n
-   x - вектор неизвестных (искомое решение)
-   b - вектор свободных членов

Входные данные:

```cpp
using InType = std::tuple<std::size_t /* n */,
               std::vector<double>, /* a */,
               std::vector<double>, /* b */,
               double /* eps */>;
```

Выходные данные:

```cpp
using OutType = std::vector<double>;
```

Ограничения:

-   n > 0
-   Матрица A должна быть невырождена (det(A) ≠ 0)
-   eps > 0
-   Метод должен сойтись при заданной точности

## 3. Последовательный алгоритм

Метод Зейделя (Gauss–Seidel) в моей реализации решает систему линейных уравнений итеративно, **последовательно** обновляя компоненты `x[i]` и используя **уже обновлённые значения** в рамках текущей итерации (in-place).

### Описание алгоритма

1. Инициализация

Вектор решения создаётся и заполняется нулями:

```cpp
std::vector<double> x(n, 0);
```

Также создаётся массив ошибок (разниц между новым и старым значением компоненты):

```cpp
std::vector<double> epsilons(n, -1);
```

2. Итерационный процесс (in-place)

Итерации выполняются, пока `iter_continue == true`:

```cpp
bool iter_continue = true;
while (iter_continue) {
  ...
}
```

На каждой итерации последовательно по `i` вычисляется новое значение `x[i]`:

```cpp
for (std::size_t i = 0; i < n; i++) {
  double ix = b[i];
  ...
  ix = ix / a[(i * n) + i];
  epsilons[i] = std::fabs(ix - x[i]);
  x[i] = ix;
}
```

#### Использование in-place массива `x`

-   Суммирование по `j < i` использует **уже обновлённые** значения `x[j]` текущей итерации (потому что `x[j]` уже был перезаписан ранее в этом же цикле по `i`):

```cpp
for (std::size_t j = 0; j < i; j++) {
  ix = ix - (a[(i * n) + j] * x[j]);
}
```

-   Суммирование по `j > i` использует **ещё не обновлённые** значения `x[j]` (они остались от предыдущего шага/итерации, т.к. `x[j]` будет обновлён позже):

```cpp
for (std::size_t j = i + 1; j < n; j++) {
  ix = ix - (a[(i * n) + j] * x[j]);
}
```

3. Проверка сходимости

После обновления всех компонент считается максимум по `epsilons` и принимается решение — продолжать или остановиться.

Вызов проверки:

```cpp
iter_continue = IterContinue(epsilons, eps);
```

Реализация критерия:

```cpp
bool KarpichISeidolMethodSEQ::IterContinue(std::vector<double> &iter_eps, double correct_eps) {
  double max_in_iter = *std::ranges::max_element(iter_eps);
  return max_in_iter > correct_eps;
}
```

То есть итерации продолжаются, пока выполняется условие:

```cpp
max_in_iter > correct_eps
```

### Алгоритмическая сложность

-   **Одна итерация**: \(O(n^2)\), так как для каждого \(i\) вычисляются суммы по элементам строки матрицы.
-   **Общая сложность**: \(O(k \cdot n^2)\), где \(k\) — число итераций до достижения заданной точности \(\varepsilon\).

## 4. Параллельный алгоритм

Параллельный алгоритм реализует **итерационный метод Зейделя** для решения системы линейных уравнений \(A x = b\) с использованием технологии **MPI**. Матрица \(A\) и вектор \(b\) распределяются между процессами по строкам, а на каждой итерации выполняется параллельное обновление компонент вектора решения \(x\) и проверка критерия остановки по максимальному изменению.

### Алгоритм работы

Параллельный алгоритм работает следующим образом:

1. Инициализация MPI окружения:

    - Определяются `rank` (ранг процесса) и `mpi_size` (число процессов)
    - Из входных данных считывается точность \(\varepsilon\)
    - Процесс с `rank = 0` хранит исходные данные: размерность \(n\), матрицу \(A\) и вектор \(b\)

2. Рассылка размерности задачи:

    - Процесс `rank = 0` задаёт \(n\)
    - Значение \(n\) рассылается всем процессам через `MPI_Bcast`

3. Распределение данных (по строкам):

    - Вычисляется базовое число строк на процесс:
        - `step = n / mpi_size`
        - `remainder = n % mpi_size`
    - Формируются массивы для `MPI_Scatterv`:
        - `send_counts_b[p]` — сколько элементов вектора \(b\) (строк) получает процесс \(p\)
        - `send_counts_a[p]` — сколько элементов матрицы \(A\) получает процесс \(p\) (то есть `send_counts_b[p] * n`)
        - `displ_b[p]`, `displ_a[p]` — смещения (displacements) в исходных массивах
    - Первые `remainder` процессов получают на одну строку больше:
        - `send_counts_b[p] = step (+1 для p < remainder)`
        - `send_counts_a[p] = send_counts_b[p] * n`
    - Рассылка:
        - локальный кусок вектора \(b\) передается в `lb` с помощью `MPI_Scatterv`
        - локальные строки матрицы \(A\) передаются в `la` с помощью `MPI_Scatterv`

4. Итерационный процесс (до достижения точности):

    - Инициализируются:
        - глобальный вектор решения `x` размера \(n\) (начальное приближение — нули)
        - локальный вектор `x_new` размера `lb.size()` (обновления текущего процесса)
        - массивы изменений (невязок по компонентам) `epsilons` (глобальный) и `epsilons_new` (локальный)
    - Пока условие продолжения истинно (`iter_continue == true`) выполняется итерация:

        1. **Локальное обновление компонент \(x\)** для строк, принадлежащих процессу:

        - Для каждой локальной строки `i`:
            - Глобальный индекс строки:
                - `row = displ_b[rank] + i`
            - Вычисляется новое значение \(x\_{row}\) по формуле Зейделя:
                - берётся правая часть `ix = lb[i]`
                - вычитаются вклады всех столбцов, кроме диагонального:
                    - для \(j < row\): используется текущее `x[j]`
                    - для \(j > row\): также используется текущее `x[j]`
                - выполняется деление на диагональный элемент:
                  \[
                  x*{row} = \frac{b*{row} - \sum*{j \ne row} a*{row,j} x*j}{a*{row,row}}
                  \]
            - Оценивается изменение компоненты:
                - `epsilons[row] = |ix - x[row]|`
            - Обновляются:
                - глобальное `x[row] = ix`
                - локальные буферы `x_new[i] = ix`, `epsilons_new[i] = epsilons[row]`

        2. **Синхронизация вектора решения между всеми процессами**:

        - Все локальные куски `x_new` собираются в общий `x` с помощью `MPI_Allgatherv`
        - Аналогично собираются локальные изменения `epsilons_new` в глобальный `epsilons` с помощью `MPI_Allgatherv`

        3. **Проверка критерия остановки**:

        - Находится максимум по всем компонентам:
            - `max_in_iter = max(epsilons)`
        - Итерации продолжаются, если:
          \[
          \max(\Delta x) > \varepsilon
          \]
          где \(\Delta x_i = |x_i^{new} - x_i^{old}|\)

5. Завершение:

-   После выхода из цикла итоговый вектор `x` записывается в выход задачи (`GetOutput() = x`)

### Особенности реализации

-   Распределение строк выполнено через `MPI_Scatterv`, что позволяет корректно обрабатывать случаи, когда \(n\) не делится на число процессов (учитывается `remainder`)
-   На каждой итерации выполняются **две коллективные операции**:
    -   `MPI_Allgatherv` для сборки обновленного решения \(x\)
    -   `MPI_Allgatherv` для сборки изменений \(|x^{new}-x^{old}|\) и проверки точности
-   Условие остановки основано на максимальном изменении компоненты (норма \(\|\Delta x\|\_\infty\))

## 5. Схема распараллеливания

Для параллельной реализации используется технология **MPI**. Распараллеливание выполняется **по строкам матрицы \(A\)** (и соответствующим элементам \(b\)). Каждый процесс отвечает за вычисление новых значений \(x_i\) только для своего диапазона строк.

### Распределение данных

1. Процесс 0 хранит полные входные данные: \(n\), \(A\), \(b\)
2. Всем процессам рассылается \(n\) через `MPI_Bcast`
3. Далее строки распределяются почти равномерно:

-   `step = n / mpi_size`
-   `remainder = n % mpi_size`
-   процессам `0..remainder-1` выдаётся по `step+1` строк
-   остальным — по `step` строк

Для процесса `p`:

-   число строк:
    -   `rows_p = send_counts_b[p]`
-   диапазон глобальных индексов строк:
    -   `start_row = displ_b[p]`
    -   `end_row = displ_b[p] + rows_p - 1`
-   локально хранятся:
    -   `lb` — кусок вектора \(b\) размера `rows_p`
    -   `la` — строки матрицы \(A\) размера `rows_p * n`

### Коммуникации между процессами

Коммуникации состоят из двух этапов: распределение данных перед стартом и синхронизация на каждой итерации.

1. **Перед началом вычислений**:

-   `MPI_Bcast(n)` — рассылка размерности
-   `MPI_Scatterv(b → lb)` — раздача частей \(b\)
-   `MPI_Scatterv(A → la)` — раздача строк матрицы \(A\)

2. **На каждой итерации метода Зейделя**:

-   каждый процесс вычисляет новые значения `x_new` для своих строк и локальные изменения `epsilons_new`
-   затем выполняются коллективные операции:
    -   `MPI_Allgatherv(x_new → x)` — все процессы получают актуальный полный вектор \(x\)
    -   `MPI_Allgatherv(epsilons_new → epsilons)` — все процессы получают изменения по всем компонентам для проверки условия остановки

### Схема работы

```text
Инициализация:
  rank 0: n, A, b
  MPI_Bcast(n)

Распределение:
  MPI_Scatterv(b) -> lb (локальные элементы b)
  MPI_Scatterv(A) -> la (локальные строки A)

Итерации (пока max(|Δx|) > eps):
  Каждый процесс p:
    для row в [displ_b[p], displ_b[p] + send_counts_b[p) ):
      вычислить x[row] (локально) по методу Зейделя
      сохранить в x_new и epsilons_new

  MPI_Allgatherv(x_new) -> x           (актуальный x на всех процессах)
  MPI_Allgatherv(epsilons_new) -> epsilons
  Проверка: max(epsilons) > eps ?

Завершение:
  x — итоговое решение
```

## 6. Детали реализации

### Структура кода

Файлы проекта:

-   seq/src/ops_seq.cpp - последовательная реализация
-   mpi/src/ops_mpi.cpp - параллельная реализация с использованием MPI
-   common/include/common.hpp - общие определения типов данных
-   tests/functional/main.cpp - функциональные тесты
-   tests/performance/main.cpp - тесты производительности

### Основные функции и методы

**Последовательная реализация:**

-   RunImpl() - основной алгоритм решения системы методом Зейделя
-   IterContinue(std::vector<double> &iter_eps, double correct_eps) - проверка критерия сходимости

**Параллельная реализация:**

-   Те же методы, но с использованием MPI для коммуникаций
-   RunImpl() дополнительно использует:
    -   MPI_Bcast для рассылки размера n
    -   MPI_Scatterv для распределения данных
    -   MPI_Allgatherv для синхронизации вектора решения и проверка критерия остановки

## 7. Экспериментальная среда

| Компонент  | Значение                                |
| ---------- | --------------------------------------- |
| CPU        | Apple M2 (8 cores)                      |
| RAM        | 16 GB                                   |
| ОС         | OS: Ubuntu 24.04 (DevContainer / Mac)   |
| Компилятор | GCC 13.3.0 (g++), C++20, CMake, Release |
| MPI        | mpirun (Open MPI) 4.1.6                 |

### Тестовые данные

1. Функциональные тесты:
    - сгенерированные системы с числом неизвестных: 4, 14 (seed = 666, eps = 1e-4)
2. Перформанс тесты:
    - сгенерированная система с числом неизвестных: 1000 (seed = 666, eps = 1e-6)

## 8. Анализ

### 8.1 Корректность

Корректность работы проверялась с помощью функциональных тестов (GTest) для двух реализаций: **SEQ** и **MPI**.

1. **Функциональные тесты**: тесты выполняются на сгенерированных СЛАУ с диагональным преобладанием:

    - `n = 4`, `seed = 666`, `eps = 1e-4`
    - `n = 14`, `seed = 666`, `eps = 1e-4`

    В тестах заранее генерируется вектор истинного решения `x`, затем формируется правая часть `b = A * x`.

### 8.2 Производительность

Производительность измерялась с помощью perf-тестов для **SEQ** и **MPI** реализаций на сгенерированной задаче:

-   `n = 1000`, `seed = 666`, `eps = 1e-6`

Обе реализации (`SEQ`, `MPI`) прошли все тесты успешно.

#### Количество неизвеcтных `2000`

| Mode | Count | Time, s | Speedup | Efficiency |
| ---- | ----- | ------- | ------- | ---------- |
| seq  | 1     | 0.062   | 1.00    | N/A        |
| mpi  | 2     | 0.062   | 1.00    | 50%        |
| mpi  | 4     | 0.062   | 1.00    | 25%        |
| mpi  | 5     | 0.064   | 0.97    | 19.4%      |
| mpi  | 8     | 0.068   | 0.91    | 11.4%      |

#### Количество неизвеcтных `5000`

| Mode | Count | Time, s | Speedup | Efficiency |
| ---- | ----- | ------- | ------- | ---------- |
| seq  | 1     | 0.67    | 1.00    | N/A        |
| mpi  | 2     | 0.358   | 1.87    | 93.5%      |
| mpi  | 4     | 0.410   | 1.63    | 40.8%      |
| mpi  | 5     | 0.440   | 1.52    | 30.5%      |
| mpi  | 8     | 0.520   | 1.29    | 16.1%      |

Анализ результатов показывает следующее:

-   **Производительность при `n = 2000`**: ускорения не наблюдается (для 2 и 4 процессов `Speedup = 1.0`), а при 5 и 8 процессах время даже увеличивается. Это указывает на то, что для такой размерности накладные расходы MPI (обмены и синхронизация) сопоставимы или выше полезных вычислений.

-   **Производительность при `n = 5000`**: на 2 процессах достигается наилучшее ускорение (`Speedup = 1.87`, `Efficiency = 93.5%`), т.е. при росте размерности задача начинает эффективнее распараллеливаться. При увеличении числа процессов до 4, 5 и 8 эффективность заметно падает, что связано с ростом коммуникационных накладных расходов и ограничениями по памяти/кэшу, поэтому масштабируемость ухудшается.

-   **Вывод**: MPI-версия даёт выигрыш только при достаточно большой задаче и умеренном числе процессов; при малых размерностях и при дальнейшем увеличении числа процессов накладные расходы начинают доминировать и снижают эффективность.

## 9. Выводы

В ходе выполнения работы мной были реализованы последовательный и MPI алгоритмы решения систем линейных уравнений методом Зейделя.

Основное:

-   Реализована последовательная версия алгоритма метода Зейделя
-   Реализована параллельная версия с использованием технологии MPI
-   Функциональные тесты покрывают работу
-   Проведен сравнительный анализ производительности

## 10. Источники

1. Сысоев А. В. Курс лекций по параллельному программированию

2. Документация Open MPI https://www.open-mpi.org/doc/

3. Microsoft Функции MPI https://learn.microsoft.com/ru-ru/message-passing-interface/mpi-functions

## Приложение

### Фрагмент кода: Последовательная реализация метода Зейделя

```cpp
bool KarpichISeidolMethodSEQ::RunImpl() {
  std::size_t n = std::get<0>(GetInput());
  std::vector<double> &a = std::get<1>(GetInput());
  std::vector<double> &b = std::get<2>(GetInput());
  double eps = std::get<3>(GetInput());

  std::vector<double> x(n, 0);
  std::vector<double> epsilons(n, -1);
  bool iter_continue = true;
  while (iter_continue) {
    for (std::size_t i = 0; i < n; i++) {
      double ix = b[i];
      for (std::size_t j = 0; j < i; j++) {
        ix = ix - (a[(i * n) + j] * x[j]);
      }
      for (std::size_t j = i + 1; j < n; j++) {
        ix = ix - (a[(i * n) + j] * x[j]);
      }
      ix = ix / a[(i * n) + i];
      epsilons[i] = std::fabs(ix - x[i]);
      x[i] = ix;
    }
    iter_continue = IterContinue(epsilons, eps);
  }
  GetOutput() = x;
  return true;
}
```

### Фрагмент кода: Параллельная реализация метода Зейделя (MPI)

```cpp
bool KarpichISeidolMethodMPI::RunImpl() {
  double eps = std::get<3>(GetInput());

  int rank = 0;
  int mpi_size = 0;
  MPI_Comm_rank(MPI_COMM_WORLD, &rank);
  MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);

  double *a = nullptr;
  double *b = nullptr;

  std::size_t n = 0;
  if (rank == 0) {
    n = std::get<0>(GetInput());
    a = std::get<1>(GetInput()).data();
    b = std::get<2>(GetInput()).data();
  }
  MPI_Bcast(&n, 1, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);

  int step = static_cast<int>(n) / mpi_size;
  int remainder = static_cast<int>(n) % mpi_size;

  std::vector<int> send_counts_a(mpi_size, static_cast<int>(step * n));
  std::vector<int> send_counts_b(mpi_size, step);
  std::vector<int> displ_a(mpi_size, 0);
  std::vector<int> displ_b(mpi_size, 0);
  for (int i = 0; i < remainder; ++i) {
    send_counts_b[i]++;
    send_counts_a[i] += static_cast<int>(n);
  }
  for (int i = 1; i < mpi_size; ++i) {
    displ_b[i] = displ_b[i - 1] + send_counts_b[i - 1];
    displ_a[i] = displ_a[i - 1] + send_counts_a[i - 1];
  }
  std::vector<double> lb(send_counts_b[rank]);
  MPI_Scatterv(b, send_counts_b.data(), displ_b.data(), MPI_DOUBLE, lb.data(), static_cast<int>(lb.size()), MPI_DOUBLE,
               0, MPI_COMM_WORLD);

  std::vector<double> la(send_counts_a[rank]);
  MPI_Scatterv(a, send_counts_a.data(), displ_a.data(), MPI_DOUBLE, la.data(), static_cast<int>(la.size()), MPI_DOUBLE,
               0, MPI_COMM_WORLD);

  std::vector<double> x(n, 0.0);
  std::vector<double> x_new(lb.size(), 0.0);
  std::vector<double> epsilons(n, 0.0);
  std::vector<double> epsilons_new(lb.size(), 0.0);
  bool iter_continue = true;
  while (iter_continue) {
    for (std::size_t i = 0; i < lb.size(); i++) {
      std::size_t row = displ_b[rank] + i;
      double ix = lb[i];
      for (std::size_t j = 0; j < row; j++) {
        ix = ix - (la[(i * n) + j] * x[j]);
      }
      for (std::size_t j = row + 1; j < n; j++) {
        ix = ix - (la[(i * n) + j] * x[j]);
      }

      ix = ix / la[(i * n) + row];
      epsilons[row] = std::fabs(ix - x[row]);
      epsilons_new[i] = epsilons[row];
      x[row] = ix;
      x_new[i] = ix;
    }
    MPI_Allgatherv(x_new.data(), static_cast<int>(x_new.size()), MPI_DOUBLE, x.data(), send_counts_b.data(),
                   displ_b.data(), MPI_DOUBLE, MPI_COMM_WORLD);

    MPI_Allgatherv(epsilons_new.data(), static_cast<int>(epsilons_new.size()), MPI_DOUBLE, epsilons.data(),
                   send_counts_b.data(), displ_b.data(), MPI_DOUBLE, MPI_COMM_WORLD);

    iter_continue = IterContinue(epsilons, eps);
  }

  GetOutput() = x;
  if (rank != 0) {
    delete[] a;
    delete[] b;
  }
  return true;
}
```
